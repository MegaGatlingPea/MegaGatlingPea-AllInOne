{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. 3d point encoder\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from coati.models.encoding.e_gcl_sparse import e_gcl_sparse # encoder layer\n",
    "from coati.common.periodic_table import XY_ONE_HOT_FULL\n",
    "\n",
    "#settings\n",
    "n_layer_e3gnn: int = 4\n",
    "n_layer_xformer: int = 16\n",
    "n_hidden_xformer: int = 256\n",
    "n_hidden_e3nn: int = 256\n",
    "msg_cutoff_e3nn: float = 4.0\n",
    "n_embd_common: int = 256\n",
    "n_head: int = 8\n",
    "n_seq: int = 200\n",
    "n_tok: int = 4\n",
    "biases: bool = True\n",
    "torch_emb: bool = False\n",
    "residual: bool = False\n",
    "norm_clips: bool = True\n",
    "norm_embed: bool = False\n",
    "token_mlp: bool = True  # Do we use a nonlinear MLP to convert HCLIP into a token.\n",
    "use_point_encoder: bool = True  # if false, do not use a point encoder at all.\n",
    "old_architecture: bool = False\n",
    "device: torch.device = torch.device(\"cpu\")\n",
    "dtype: torch.dtype = torch.float\n",
    "embed_dim = n_embd_common\n",
    "\n",
    "#encoder function\n",
    "class e3gnn_clip(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_node_nf: int = len(XY_ONE_HOT_FULL(1)),\n",
    "        hidden_nf: int = 128,\n",
    "        device: str = \"cpu\",\n",
    "        act_fn: str = \"SiLU\",\n",
    "        n_layers: int = 5,\n",
    "        instance_norm: bool = True,\n",
    "        message_cutoff: int = 5,\n",
    "        dtype=torch.float,\n",
    "        torch_emb: bool = False,\n",
    "        residual: bool = False,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        The Welling research code is quadratic in batch size.\n",
    "        and has no instancenorm. This fixes that.\n",
    "        This also has no edge feature b/c bonds aren't real\n",
    "\n",
    "        h_l => n_graph X n_node X n_hidden_features\n",
    "        x_l => n_graph X n_node X n_dim\n",
    "        e_ij => n_graph X n_node X n_node X n_edge_features\n",
    "\n",
    "        Args:\n",
    "            in_node_nf: number of input features for each node (atom)\n",
    "            in_edge_nf: number of input featuers for each edge (bond)\n",
    "            hidden_nf: dimension of the hidden representation (per atom)\n",
    "            code_nf: dimension of a code conditioning the final aggregation. (optional)\n",
    "            residual_feature: whether to include residual-like h0 in the node_model\n",
    "        \"\"\"\n",
    "        super(e3gnn_clip, self).__init__()\n",
    "        self.dtype = dtype\n",
    "        self.hidden_nf = hidden_nf\n",
    "\n",
    "        if not torch_emb:\n",
    "            self.torch_emb = False\n",
    "            self.in_node_nf = in_node_nf\n",
    "            self.emb = None\n",
    "        else:\n",
    "            self.torch_emb = True\n",
    "            self.in_node_nf = hidden_nf\n",
    "            self.emb = nn.Embedding(84, self.hidden_nf, device=device, dtype=dtype)\n",
    "\n",
    "        self.device = device\n",
    "        self.n_layers = n_layers\n",
    "        self.instance_norm = instance_norm\n",
    "        self.message_cutoff = torch.tensor(message_cutoff, requires_grad=False)\n",
    "\n",
    "        assert dropout >= 0.0 and dropout < 1.0\n",
    "        self.dropout = dropout\n",
    "\n",
    "        if act_fn == \"SiLU\":\n",
    "            self.act_fn = nn.SiLU()\n",
    "        elif act_fn == \"GELU\":\n",
    "            self.act_fn = nn.GELU()\n",
    "        else:\n",
    "            raise Exception(\"Bad act_fn\")\n",
    "\n",
    "        ### Encoder\n",
    "        if self.torch_emb:\n",
    "            self.embedding = torch.nn.Identity()\n",
    "        else:\n",
    "            self.embedding = nn.Linear(self.in_node_nf, hidden_nf)\n",
    "\n",
    "        if instance_norm:\n",
    "            self.embedding_norm = torch.nn.InstanceNorm1d(hidden_nf)\n",
    "        else:\n",
    "            self.embedding_norm = torch.nn.Identity()\n",
    "\n",
    "        self.node_dec = nn.Sequential(\n",
    "            nn.Linear(self.hidden_nf, self.hidden_nf),\n",
    "            self.act_fn,\n",
    "            nn.Dropout(p=self.dropout) if self.dropout else nn.Identity(),\n",
    "            nn.Linear(self.hidden_nf, self.hidden_nf),\n",
    "        )\n",
    "\n",
    "        for i in range(0, n_layers):\n",
    "            self.add_module(\n",
    "                \"gcl_%d\" % i,\n",
    "                e_gcl_sparse(\n",
    "                    self.hidden_nf,\n",
    "                    act_fn=self.act_fn,\n",
    "                    residual=residual,\n",
    "                    attention=False,\n",
    "                    instance_norm=instance_norm,\n",
    "                    residual_nf=(in_node_nf if residual else 0),\n",
    "                    dropout=dropout,\n",
    "                    prop_coords=False,\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, atoms: torch.Tensor, coords: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        atoms: batch X max_n_atom long tensor of atomic numbers.\n",
    "        coords: node coordinates.\n",
    "        \"\"\"\n",
    "        if self.torch_emb:\n",
    "            assert (atoms > 84).sum().detach().item() == 0\n",
    "            nodes = self.emb(atoms)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                ans = atoms.tolist()\n",
    "                nodes = torch.tensor(\n",
    "                    [[XY_ONE_HOT_FULL(int(atom)) for atom in mol] for mol in ans],  #transform atomic number into atomic table one-hot(x = 18, y = 10)\n",
    "                    dtype=torch.float32,\n",
    "                    device=atoms.device,\n",
    "                    requires_grad=False,\n",
    "                )\n",
    "        node_mask = (atoms > 0).to(atoms.device, torch.float)\n",
    "        assert nodes.isfinite().all()\n",
    "        assert coords.isfinite().all()\n",
    "        assert node_mask.isfinite().all()\n",
    "        # print('nodes', nodes)\n",
    "        # bsize x n_atoms x hidden_nf\n",
    "        # print('emb nodes', self.embedding(nodes).size())\n",
    "        h = self.embedding_norm(self.embedding(nodes))\n",
    "        for i in range(0, self.n_layers):\n",
    "            h, _ = self._modules[\"gcl_%d\" % i](h, coords, node_mask, h0=nodes)\n",
    "        h = self.node_dec(h)\n",
    "        h = h * node_mask.unsqueeze(-1)\n",
    "        natoms = torch.maximum(node_mask.sum(-1), torch.ones_like(node_mask.sum(-1)))\n",
    "        h = torch.sum(h, dim=1) / natoms.unsqueeze(-1)\n",
    "        return h\n",
    "\n",
    "\n",
    "#encoder\n",
    "point_encoder = e3gnn_clip(\n",
    "    device=device,\n",
    "    dtype=dtype,\n",
    "    hidden_nf=n_hidden_e3nn,\n",
    "    message_cutoff=msg_cutoff_e3nn,\n",
    "    dropout=0.0,\n",
    "    torch_emb=torch_emb,\n",
    "    residual=residual,\n",
    "    n_layers=n_layer_e3gnn,\n",
    ")\n",
    "\n",
    "#norm\n",
    "point_to_clip = nn.Sequential(\n",
    "        nn.LayerNorm(point_encoder.hidden_nf),\n",
    "        nn.Linear(point_encoder.hidden_nf, embed_dim),\n",
    "    )\n",
    "\n",
    "#full encoder\n",
    "def encode_points(atoms: torch.Tensor, coords: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Embeds coordinates/atoms and projects into the latent space.\n",
    "\n",
    "    If point encoder is not used, this returns a zero tensor.\n",
    "    \"\"\"\n",
    "    if use_point_encoder:\n",
    "        return point_to_clip(point_encoder(atoms, coords))\n",
    "    else:\n",
    "        return torch.zeros(atoms.shape[0], embed_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.8823, 0.9150, 0.3829],\n",
      "         [0.9593, 0.3904, 0.6009]],\n",
      "\n",
      "        [[0.2566, 0.7936, 0.9408],\n",
      "         [0.1332, 0.9346, 0.5936]]]) \n",
      " tensor([[3, 1],\n",
      "        [4, 5]])\n",
      "ans [[3, 1], [4, 5]]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "nodes tensor([[[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "          0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]]])\n",
      "emb nodes torch.Size([2, 2, 128])\n",
      "torch.Size([2, 128])\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "b = 2\n",
    "n = 2\n",
    "d = 3\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "mol_coords = torch.rand((b, n, d))\n",
    "mol_atom = torch.randint(1, 7, (b, n))\n",
    "print(mol_coords, '\\n', mol_atom)\n",
    "h_points = encode_points(mol_atom, mol_coords)\n",
    "print(h_points.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. ecloud encoder\n",
    "from typing import List, Union\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#settings\n",
    "n_layer_e3gnn: int = 4\n",
    "n_layer_xformer: int = 16\n",
    "n_hidden_xformer: int = 256\n",
    "n_hidden_e3nn: int = 256\n",
    "msg_cutoff_e3nn: float = 4.0\n",
    "n_embd_common: int = 256\n",
    "n_head: int = 8\n",
    "n_seq: int = 200\n",
    "n_tok: int = 4\n",
    "biases: bool = True\n",
    "torch_emb: bool = False\n",
    "residual: bool = False\n",
    "norm_clips: bool = True\n",
    "norm_embed: bool = False\n",
    "token_mlp: bool = True  # Do we use a nonlinear MLP to convert HCLIP into a token.\n",
    "use_point_encoder: bool = True  # if false, do not use a point encoder at all.\n",
    "old_architecture: bool = False\n",
    "device: torch.device = torch.device(\"cpu\")\n",
    "dtype: torch.dtype = torch.float\n",
    "embed_dim = n_embd_common\n",
    "\n",
    "#encoder layer\n",
    "class Conv3DEncoder(nn.Module):\n",
    "    def __init__(self, in_channels=1, kernel_size=3, stride=1, padding=1, dilation=1, d_model=256):\n",
    "        super(Conv3DEncoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.conv1 = nn.Conv3d(in_channels, d_model // 4, kernel_size, stride, padding, dilation)\n",
    "        self.conv2 = nn.Conv3d(d_model // 4, d_model // 2, kernel_size, stride, padding, dilation)\n",
    "        self.conv3 = nn.Conv3d(d_model// 2, d_model, kernel_size, stride, padding, dilation)\n",
    "        self.pool = nn.MaxPool3d(kernel_size=2, stride=2)\n",
    "        self.relu = nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        bz = x.size(0)\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(bz, -1, self.d_model)\n",
    "        sl = x.size(1)\n",
    "        # prepare input for decoder\n",
    "        x = x.transpose(0, 1)\n",
    "        \n",
    "\n",
    "        src_padding_mask = torch.zeros((bz, sl), dtype=torch.bool).to(x.device)\n",
    "\n",
    "        # encoder_out = x, src_padding_mask\n",
    "        # return encoder_out\n",
    "        return x\n",
    "\n",
    "#norm\n",
    "eclouds_to_clip = nn.Sequential(\n",
    "    nn.LayerNorm(64 * 768),\n",
    "    nn.Linear(64 * 768, embed_dim),\n",
    ")\n",
    "\n",
    "#full ecloud_encoder\n",
    "ecloud_encoder = Conv3DEncoder(d_model=768)\n",
    "def encode_eclouds(eclouds: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Embeds eclouds into the latent space.\n",
    "\n",
    "    \"\"\"\n",
    "    ecloud_emb = ecloud_encoder(eclouds).transpose(0, 1) # (b, s, 768)\n",
    "    ecloud_emb = ecloud_emb.reshape(ecloud_emb.shape[0], -1) # (b, s*768)\n",
    "    return eclouds_to_clip(ecloud_emb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.Size([2, 128])\n"
     ]
    }
   ],
   "source": [
    "b = 2\n",
    "x = 32\n",
    "y = 32\n",
    "z = 32\n",
    "mol_ecloud = torch.rand((b, x, y, z))\n",
    "print(mol_ecloud.dtype)\n",
    "h_ecloud = encode_eclouds(mol_ecloud)\n",
    "print(h_ecloud.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converged SCF energy = -152.127991065501\n",
      "atom info [{'atomic_number': [6, 6, 8, 1, 1, 1, 1, 1, 1], 'coordinates': [[-1.723572, 0.319103, 0.094505], [0.881941, -0.895013, -0.07507], [2.613345, 0.78721, -1.164387], [-2.380077, 0.880869, -1.81821], [-3.089854, -1.049592, 0.908722], [-1.649145, 2.017366, 1.325404], [1.527014, -1.4826, 1.840037], [0.761771, -2.611674, -1.275931], [3.058577, 2.03433, 0.164931]]}]\n",
      "converged SCF energy = -152.128702196742\n",
      "atom info [{'atomic_number': [6, 6, 8, 1, 1, 1, 1, 1, 1], 'coordinates': [[-1.890702, -0.202512, 0.39852], [0.927044, -0.368813, -0.142042], [1.886061, 2.052294, -0.608457], [-2.22926, 1.002338, 2.083537], [-2.891573, 0.622128, -1.251873], [-2.655278, -2.121252, 0.769064], [1.899849, -1.215916, 1.51865], [1.236723, -1.596591, -1.820879], [3.717136, 1.828325, -0.946519]]}]\n"
     ]
    }
   ],
   "source": [
    "#generate encoder example\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from pyscf import gto, scf, tools\n",
    "import numpy as np\n",
    "from scipy.ndimage import zoom\n",
    "import numpy as np\n",
    "import torch\n",
    "import h5py\n",
    "from coati.models.encoding.tokenizers.trie_tokenizer import TrieTokenizer\n",
    "from coati.models.encoding.tokenizers import get_vocab\n",
    "\n",
    "def read_cube_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    atom_line = lines[2].split()     # 跳过前两行（注释行）\n",
    "    num_atoms = int(atom_line[0])    # 获取原子数和原点坐标\n",
    "\n",
    "    grid_info = [list(map(float, lines[i].split())) for i in range(3, 6)]     # 第三行到第五行分别是网格信息\n",
    "    grid_shape = [int(abs(info[0])) for info in grid_info]  # 获取网格维度\n",
    "    \n",
    "    origin = np.array(grid_info)[:, 1:4]     # 网格大小和原点\n",
    "    \n",
    "    # atom_info = [list(map(float, lines[i].split())) for i in range(6, 6 + num_atoms)]     # 原子信息 (读取接下来 num_atoms 行的数据)\n",
    "    atom_info = []\n",
    "    mol_coords = []\n",
    "    mol_atomic = []\n",
    "    for i in range(6, 6 + num_atoms):\n",
    "        line_data = list(map(float, lines[i].split()))\n",
    "        atomic_number = int(line_data[0])  # 原子序号\n",
    "        coordinates = line_data[2:5]  # x, y, z 坐标\n",
    "        mol_coords.append(coordinates)\n",
    "        mol_atomic.append(atomic_number)\n",
    "    atom_info.append({\n",
    "        'atomic_number': mol_atomic,\n",
    "        'coordinates': mol_coords\n",
    "    })\n",
    "    density_data = []\n",
    "    for line in lines[6 + num_atoms:]:\n",
    "        density_data.extend(map(float, line.split()))\n",
    "    \n",
    "    density_data = np.array(density_data).reshape(grid_shape)\n",
    "    \n",
    "    return density_data, atom_info, origin, grid_shape\n",
    "\n",
    "def sml2ecloud(smiles):\n",
    "    '''\n",
    "    smiles: list of smile stringa\n",
    "    return: h5 file, \n",
    "    '''\n",
    "    eclouds = torch.tensor([])\n",
    "    atomic_number = torch.tensor([])\n",
    "    coords = torch.tensor([])\n",
    "    augmented_tokens = torch.tensor([])\n",
    "    tokenizer = TrieTokenizer(n_seq=128, **get_vocab('mar'))\n",
    "    for smile in smiles:\n",
    "        mol = Chem.MolFromSmiles(smile)\n",
    "\n",
    "        # mol to points\n",
    "        mol = Chem.AddHs(mol)  # 添加氢原子\n",
    "        AllChem.EmbedMolecule(mol)  # 生成3D坐标\n",
    "        AllChem.UFFOptimizeMolecule(mol)  # 用UFF力场优化\n",
    "        \n",
    "        conf = mol.GetConformer()\n",
    "        xyz = \"\"\n",
    "        for i, atom in enumerate(mol.GetAtoms()):\n",
    "            pos = conf.GetAtomPosition(i)\n",
    "            xyz += f\"{atom.GetSymbol()} {pos.x} {pos.y} {pos.z}\\n\"\n",
    "\n",
    "        mol = gto.M(atom=xyz, basis=\"sto-3g\")  # 定义分子并选择基组\n",
    "        mf = scf.RHF(mol)  # Hartree-Fock计算\n",
    "        mf.kernel()  # 运行计算\n",
    "\n",
    "        # 生成电子密度\n",
    "        tools.cubegen.density(mol, f'examples/eclouds_{smile}.cube', mf.make_rdm1())\n",
    "        # # 添加 batch 维度和 channel 维度，形状为 (1, 1, x, y, z)\n",
    "        # density_tensor = density_tensor.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "        # print(density_tensor.shape)  # 输出形状为 (1, 1, x, y, z)\n",
    "\n",
    "        # print(\"电子密度网格形状：\", density_data.shape)\n",
    "        # print(\"原子信息：\", atom_info)\n",
    "        # print(\"网格原点：\", origin)\n",
    "\n",
    "        ecloud_density, atom_info, origin, grid_shape = read_cube_file(f'examples/eclouds_{smile}.cube')\n",
    "        print('atom info', atom_info)\n",
    "        n = ecloud_density.shape[0]  # get size of raw ecloud\n",
    "        target_shape = (32, 32, 32) # 使用 scipy 的 zoom 函数将电子密度插值到 (32, 32, 32)\n",
    "        ecloud = zoom(ecloud_density, (target_shape[0] / n, target_shape[1] / n, target_shape[2] / n))\n",
    "        \n",
    "        ecloud = torch.tensor(ecloud, dtype=torch.double).unsqueeze(0)\n",
    "        mol_atomic = torch.tensor(atom_info[0]['atomic_number']).unsqueeze(0)\n",
    "        mol_coords = torch.tensor(atom_info[0]['coordinates']).unsqueeze(0)\n",
    "        augmented_token = torch.tensor(tokenizer.tokenize_text(\"[CLIP][UNK][SMILES][SUFFIX][MIDDLE]\" + smile + \"[STOP]\", pad=True)).unsqueeze(0)\n",
    "        \n",
    "        eclouds = torch.cat((eclouds, ecloud), dim=0)\n",
    "        atomic_number = torch.cat((atomic_number, mol_atomic), dim=0)\n",
    "        coords = torch.cat((coords, mol_coords), dim=0)\n",
    "        augmented_tokens = torch.cat((augmented_tokens, augmented_token))\n",
    "    # 假设 density_32 是 (32, 32, 32) 的 3D 电子云数据\n",
    "    with h5py.File(f'examples/eclouds_{len(smiles)}.h5', 'w') as f:\n",
    "        f.create_dataset('ecloud', data=eclouds)\n",
    "        f.create_dataset('atomic_number', data=atomic_number)\n",
    "        f.create_dataset('coords', data=coords)\n",
    "        f.create_dataset('smiles', data=smiles)\n",
    "        f.create_dataset('augmented_tokens', data=augmented_tokens)\n",
    "smiles = \"CCO\"  # 乙醇\n",
    "smiles = [smiles, smiles]\n",
    "sml2ecloud(smiles)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ecloud torch.Size([2, 32, 32, 32])\n",
      "atomic number torch.Size([2, 9])\n",
      "coords torch.Size([2, 9, 3])\n",
      "augmented tokens torch.Size([2, 128])\n",
      "hidden molecular points: torch.Size([2, 256])\n",
      "hidden molecular eclouds: torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "#load data\n",
    "with h5py.File(f'examples/eclouds_{len(smiles)}.h5', 'r') as f:\n",
    "    eclouds = torch.tensor(f['ecloud'][:])\n",
    "    atomic_number = torch.tensor(f['atomic_number'][:])\n",
    "    coords = torch.tensor(f['coords'][:])\n",
    "    augmented_tokens = torch.tensor(f['augmented_tokens'][:]).int()\n",
    "print('ecloud', eclouds.size())\n",
    "print('atomic number', atomic_number.size())\n",
    "print('coords', coords.size())\n",
    "print('augmented tokens', augmented_tokens.size())\n",
    "h_points = encode_points(atomic_number, coords)\n",
    "eclouds = eclouds.float()\n",
    "h_ecloud = encode_eclouds(eclouds)\n",
    "print(f'hidden molecular points: {h_points.size()}')\n",
    "print(f'hidden molecular eclouds: {h_ecloud.size()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#e3gnn_eclouds_clip_e2e\n",
    "import random\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import autocast\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from coati.containers.rdkit_utils import disable_logger, permute_smiles\n",
    "from coati.models.encoding.e3gnn_clip import e3gnn_clip\n",
    "from coati.models.encoding.fill_in_middle import adj_mat_to_tokens\n",
    "from coati.models.encoding.smiles_xformer import (\n",
    "    RotarySmilesTransformer,\n",
    "    SmilesTransformerConfig,\n",
    ")\n",
    "from coati.models.encoding.tokenizers.trie_tokenizer import TrieTokenizer\n",
    "from coati.models.encoding.prefix_encoder import Conv3DEncoder\n",
    "from rdkit import Chem\n",
    "from coati.models.encoding.clip_e2e import clip_loss\n",
    "class e3gnn_eclouds_clip_e2e(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_layer_e3gnn: int = 4,\n",
    "        n_layer_xformer: int = 16,\n",
    "        n_hidden_xformer: int = 128,\n",
    "        n_hidden_e3nn: int = 128,\n",
    "        msg_cutoff_e3nn: float = 4.0,\n",
    "        n_embd_common: int = 128,\n",
    "        n_head: int = 8,\n",
    "        n_seq: int = 200,\n",
    "        n_tok: int = 4,\n",
    "        biases: bool = True,\n",
    "        torch_emb: bool = False,\n",
    "        residual: bool = False,\n",
    "        norm_clips: bool = True,\n",
    "        norm_embed: bool = False,\n",
    "        token_mlp: bool = True,  # Do we use a nonlinear MLP to convert HCLIP into a token.\n",
    "        use_point_encoder: bool = True,  # if false, do not use a point encoder at all.\n",
    "        old_architecture: bool = False,\n",
    "        device: torch.device = torch.device(\"cpu\"),\n",
    "        dtype: torch.dtype = torch.float,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embed_dim = n_embd_common\n",
    "        self.point_encoder = e3gnn_clip(\n",
    "            device=device,\n",
    "            dtype=dtype,\n",
    "            hidden_nf=n_hidden_e3nn,\n",
    "            message_cutoff=msg_cutoff_e3nn,\n",
    "            dropout=0.0,\n",
    "            torch_emb=torch_emb,\n",
    "            residual=residual,\n",
    "            n_layers=n_layer_e3gnn,\n",
    "        )\n",
    "        kwargs = {\n",
    "            \"n_layer\": n_layer_xformer,\n",
    "            \"n_embd\": n_hidden_xformer,\n",
    "            \"n_head\": n_head,\n",
    "            \"n_seq\": n_seq,\n",
    "            \"n_tok\": n_tok,\n",
    "            \"device\": device,\n",
    "            \"dtype\": dtype,\n",
    "            \"biases\": biases,\n",
    "            \"norm_embed\": norm_embed,\n",
    "        }\n",
    "        self.xformer_config = SmilesTransformerConfig(**kwargs)\n",
    "        self.xformer = RotarySmilesTransformer(self.xformer_config)\n",
    "        self.device = device\n",
    "        self.use_point_encoder = use_point_encoder\n",
    "        # Each of these get a linear mapping into the common hidden space.\n",
    "        self.ecloud_encoder = Conv3DEncoder(d_model=768)\n",
    "        if norm_clips:\n",
    "            if old_architecture:\n",
    "                self.point_to_clip = nn.Sequential(\n",
    "                    nn.Linear(self.point_encoder.hidden_nf, self.embed_dim),\n",
    "                    nn.LayerNorm(self.point_encoder.hidden_nf),\n",
    "                )\n",
    "                self.smiles_to_clip = nn.Sequential(\n",
    "                    nn.Linear(self.xformer.n_embd, self.embed_dim),\n",
    "                    nn.LayerNorm(self.embed_dim),\n",
    "                )\n",
    "            else:\n",
    "                self.point_to_clip = nn.Sequential(\n",
    "                    nn.LayerNorm(self.point_encoder.hidden_nf),\n",
    "                    nn.Linear(self.point_encoder.hidden_nf, self.embed_dim),\n",
    "                )\n",
    "                self.smiles_to_clip = nn.Sequential(\n",
    "                    nn.LayerNorm(self.embed_dim),\n",
    "                    nn.Linear(self.xformer.n_embd, self.embed_dim),\n",
    "                )\n",
    "                self.eclouds_to_clip = nn.Sequential(\n",
    "                    nn.LayerNorm(64 * 768),\n",
    "                    nn.Linear(64 * 768, self.embed_dim),\n",
    "                )\n",
    "        else:\n",
    "            self.point_to_clip = nn.Linear(self.point_encoder.hidden_nf, self.embed_dim)\n",
    "            self.smiles_to_clip = nn.Linear(self.xformer.n_embd, self.embed_dim)\n",
    "\n",
    "        if token_mlp:\n",
    "            # A mapping to make the special token(s?).\n",
    "            self.point_clip_to_special_tokens = nn.Sequential(\n",
    "                nn.SiLU(), nn.Linear(self.embed_dim, self.embed_dim)\n",
    "            )\n",
    "        else:\n",
    "            self.point_clip_to_special_tokens = nn.Identity()\n",
    "\n",
    "        n_params_e3gnn = sum(p.numel() for p in self.point_encoder.parameters())\n",
    "        n_params_smiles = sum(p.numel() for p in self.xformer.parameters())\n",
    "        n_params = n_params_e3gnn + n_params_smiles\n",
    "        print(\n",
    "            f\"number of parameters Total: {n_params_e3gnn/1e6:.2f}M xformer: {n_params_smiles/1e6:.2f}M Total: {n_params/1e6:.2f}M \"\n",
    "        )\n",
    "        self.clip_loss = clip_loss()\n",
    "        self.to(self.device)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def forward_dist(\n",
    "        self,\n",
    "        eclouds: torch.Tensor, \n",
    "        augmented_tokens: torch.Tensor,\n",
    "        atoms: torch.Tensor,\n",
    "        coords: torch.Tensor,\n",
    "        tokenizer,\n",
    "        p_clip_emb_smi: float = 0.4,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Same as the below routine but for DistributedDataParallel training.\n",
    "        \"\"\"\n",
    "        with autocast(enabled=False, device_type=\"cuda\"):\n",
    "            h_e3gnn = encode_points(atoms, coords)\n",
    "            h_eclouds = encode_eclouds(eclouds)\n",
    "            try:\n",
    "                assert h_e3gnn.shape[0] == h_eclouds.shape[0]\n",
    "            except Exception as Ex:\n",
    "                print(\n",
    "                    Ex,\n",
    "                    augmented_tokens.shape,\n",
    "                    atoms.shape,\n",
    "                    coords.shape,\n",
    "                    h_e3gnn.shape,\n",
    "                    h_eclouds.shape,\n",
    "                )\n",
    "                raise Ex\n",
    "            # print('h_e3gnn', h_e3gnn.size())\n",
    "            # print('h_ecloud', h_eclouds.size())\n",
    "            point_clip_token = self.point_clip_to_special_tokens(h_e3gnn)\n",
    "            eclouds_clip_token = self.point_clip_to_special_tokens(h_eclouds)\n",
    "            clip_token = torch.where(\n",
    "                (torch.rand((h_e3gnn.shape[0],), device=atoms.device) > p_clip_emb_smi)\n",
    "                .unsqueeze(-1)\n",
    "                .repeat(1, point_clip_token.shape[-1]),\n",
    "                point_clip_token,\n",
    "                eclouds_clip_token,\n",
    "            )\n",
    "        logits = self.xformer.forward_with_replacement(     #重新设计吧应该要\n",
    "            augmented_tokens, clip_token, tokenizer\n",
    "        )\n",
    "        bad_rows = augmented_tokens.sum(-1) < 1\n",
    "        return h_e3gnn, h_eclouds, logits, bad_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#args\n",
    "import argparse\n",
    "from coati.common.util import makedir, utc_epoch_now\n",
    "def do_args():\n",
    "    parser = argparse.ArgumentParser(description=\"token_transformer\")\n",
    "    parser.add_argument(\"--exp_name\", type=str, default=\"token_transformer\")\n",
    "    parser.add_argument(\"--run_name\", type=str, default=str(int(utc_epoch_now())))\n",
    "    parser.add_argument(\"--output_dir\", type=str, default=\"COATI_outputs\")\n",
    "    parser.add_argument(\"--model_dir\", type=str, default=\"COATI_models\")\n",
    "    parser.add_argument(\"--data_dir\", type=str, default=\"COATI_data\")\n",
    "\n",
    "    # ddp options.\n",
    "    parser.add_argument(\n",
    "        \"-ws\", \"--world_size\", default=1, type=int, help=\"total number of processes\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-nr\", \"--nr\", default=0, type=int, help=\"ranking within the nodes\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-n\", \"--nodes\", default=1, type=int, metavar=\"N\", help=\"number of nodes\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-g\",\n",
    "        \"--gpus\",\n",
    "        default=torch.cuda.device_count(),\n",
    "        type=int,\n",
    "        help=\"number of gpus per node\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--device\", type=str, default=\"cuda\", help=\"pytorch backend device.\"\n",
    "    )\n",
    "    parser.add_argument(\"--dtype\", type=str, default=\"float\", help=\"default data type\")\n",
    "    parser.add_argument(\"--log_batch_loss\", default=25, help=\"steps per tnet log\")\n",
    "    parser.add_argument(\n",
    "        \"--code_features\",\n",
    "        default=[\"protein\", \"secondary\", \"library\"],\n",
    "        help=\"one hot encoded additional dimensions.\",\n",
    "    )\n",
    "    parser.add_argument(\"--n_epochs\", type=int, default=2)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=32)\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--recipe\",\n",
    "        type=list,\n",
    "        default=[\n",
    "            {\"collection\": \"geom_drugs\", \"n_samples\": 6_000_000, \"filter\": {}},\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\"--n_layer_e3gnn\", type=int, default=4)\n",
    "    parser.add_argument(\"--n_hidden_e3nn\", type=int, default=128)\n",
    "    parser.add_argument(\"--msg_cutoff_e3nn\", type=float, default=10.0)\n",
    "    parser.add_argument(\"--n_hidden_xformer\", type=int, default=128)\n",
    "    parser.add_argument(\"--n_embd_common\", type=int, default=128)\n",
    "    parser.add_argument(\"--n_layer_xformer\", type=int, default=16)\n",
    "    parser.add_argument(\"--n_head\", type=int, default=8)\n",
    "    parser.add_argument(\n",
    "        \"--biases\", type=bool, default=True, help=\"Use biases in the xformer.\"\n",
    "    )\n",
    "    parser.add_argument(\"--n_seq\", type=int, default=200)\n",
    "    parser.add_argument(\"--tokenizer_vocab\", type=str, default=\"Jan8\")\n",
    "    parser.add_argument(\"--torch_emb\", type=bool, default=False)\n",
    "    parser.add_argument(\n",
    "        \"--load_transformer_only\",\n",
    "        type=bool,\n",
    "        default=False,\n",
    "        help=\"load trained transformer but use fresh point encoder\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\"--p_dataset\", type=float, default=0.3)\n",
    "    parser.add_argument(\"--p_formula\", type=float, default=0.3)\n",
    "    parser.add_argument(\"--p_fim\", type=float, default=0.5)\n",
    "    parser.add_argument(\"--p_graph\", type=float, default=0.3)\n",
    "    parser.add_argument(\"--p_clip\", type=float, default=0.3)\n",
    "    parser.add_argument(\"--p_clip_cut\", type=float, default=0.3)\n",
    "\n",
    "    parser.add_argument(\"--p_clip_emb_smi\", type=float, default=0.4)\n",
    "    parser.add_argument(\"--p_randsmiles\", type=float, default=0.5)\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--norm_clips\", type=bool, default=False, help=\"normalize the clip vectors\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--token_mlp\",\n",
    "        type=bool,\n",
    "        default=False,\n",
    "        help=\"Do we use an MLP or just hclip as a token.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--norm_embed\", type=bool, default=False, help=\"Layernorm after embedding\"\n",
    "    )\n",
    "    parser.add_argument(\"--weight_decay\", type=float, default=0.1)\n",
    "    parser.add_argument(\"--lr\", type=float, default=4e-4)\n",
    "    parser.add_argument(\"--clip_grad\", type=float, default=10.0)\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--do_clip\",\n",
    "        type=bool,\n",
    "        default=True,\n",
    "        help=\"If false, do not use clip loss during training.\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--test_frac\", type=float, default=0.02, help=\"test data fraction\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--valid_frac\", type=float, default=0.02, help=\"test data fraction\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--test_interval\",\n",
    "        type=int,\n",
    "        default=1,\n",
    "        metavar=\"N\",\n",
    "        help=\"how many epochs to wait before logging test\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--log_interval\",\n",
    "        type=int,\n",
    "        default=100,\n",
    "        metavar=\"N\",\n",
    "        help=\"how many batches to wait before logging training status\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--ngrad_to_save\", default=2e6, help=\"ngrad updates between model saves.\"\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--resume_document\", default=None, help=\"Restore from an S3 document\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--resume_optimizer\",\n",
    "        type=bool,\n",
    "        default=False,\n",
    "        help=\"Restore opt. from an S3 document\",\n",
    "    )\n",
    "\n",
    "    args, unparsed_args = parser.parse_known_args()\n",
    "    # args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "    if len(unparsed_args):\n",
    "        print(\"Warning... unparsed: \", unparsed_args)\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning... unparsed:  ['--f=/home/zhanght/.local/share/jupyter/runtime/kernel-v30885b7310b0657c4923c515e27dee4779e705edf.json']\n",
      "number of parameters: 12.64M\n",
      "number of parameters Total: 2.44M xformer: 19.60M Total: 22.04M \n",
      "h_e3gnn torch.Size([2, 256])\n",
      "h_xformer torch.Size([2, 256])\n",
      "logits torch.Size([2, 128, 13603])\n",
      "bad_rows torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "#forward\n",
    "args = do_args()\n",
    "args.nodes = 1  # total num nodes.\n",
    "args.nr = 0  # rank of this node.\n",
    "# note args.gpus will default to the # gpus on this node.\n",
    "args.data_parallel = True\n",
    "\n",
    "args.test_frac = 0.02\n",
    "args.valid_frac = 0.0\n",
    "args.n_layer_e3gnn = 5\n",
    "args.n_hidden_e3nn = 256\n",
    "args.msg_cutoff_e3nn = 12.0\n",
    "args.n_hidden_xformer = 256\n",
    "args.n_embd_common = 256\n",
    "args.n_layer_xformer = 16\n",
    "args.n_head = 16\n",
    "args.max_n_seq = 250  # max the model can forward\n",
    "#    args.n_seq = 90 # max allowed in training.\n",
    "args.n_seq = 80  # max allowed in training.\n",
    "args.biases = True\n",
    "args.torch_emb = False\n",
    "args.norm_clips = True\n",
    "args.norm_embed = False\n",
    "args.token_mlp = True\n",
    "\n",
    "args.tokenizer_vocab = \"mar\"\n",
    "args.p_dataset = 0.2\n",
    "args.p_formula = 0.0\n",
    "args.p_fim = 0.0\n",
    "args.p_graph = 0.0\n",
    "args.p_clip = 0.9\n",
    "args.p_clip_emb_smi = 0.5\n",
    "args.p_randsmiles = 0.3\n",
    "args.batch_size = 160\n",
    "\n",
    "args.online = False  # Possible offline training of an end-to-end clip\n",
    "args.lr = 5.0e-4\n",
    "args.weight_decay = 0.1\n",
    "\n",
    "args.dtype = \"float\"\n",
    "args.n_epochs = 25\n",
    "args.clip_grad = 10\n",
    "args.test_interval = 2\n",
    "args.debug = False\n",
    "\n",
    "args.resume_optimizer = False\n",
    "# resume from checkpoint file\n",
    "# args.resume_document = ''\n",
    "\n",
    "args.ngrad_to_save = 2e6\n",
    "\n",
    "# output logs\n",
    "args.output_dir = \"./logs/\"\n",
    "# where to save model checkpoints\n",
    "args.model_dir = \"./model_ckpts/\"\n",
    "# where to save dataset cache\n",
    "args.data_dir = \"./\"\n",
    "args.model_filename = \"coati_grande\"\n",
    "from coati.models.encoding.tokenizers import get_vocab\n",
    "from coati.models.encoding.clip_e2e import clip_loss as clip_loss_module\n",
    "# tokenizer = TrieTokenizer(n_seq=args.n_seq, **get_vocab(args.tokenizer_vocab)) 'mar'\n",
    "tokenizer = TrieTokenizer(n_seq=args.n_seq, **get_vocab('mar'))\n",
    "token_entropy_unit = np.log(float(len(tokenizer.keys))) / np.log(2.0)\n",
    "kwargs = {\n",
    "    \"n_layer_xformer\": args.n_layer_xformer,\n",
    "    \"n_layer_e3gnn\": args.n_layer_e3gnn,\n",
    "    \"n_hidden_e3nn\": args.n_hidden_e3nn,\n",
    "    \"n_hidden_xformer\": args.n_hidden_xformer,\n",
    "    \"n_embd_common\": args.n_embd_common,\n",
    "    \"biases\": args.biases,\n",
    "    \"n_head\": args.n_head,\n",
    "    \"n_seq\": args.max_n_seq,\n",
    "    \"n_tok\": tokenizer.n_token,  # base\n",
    "    \"torch_emb\": args.torch_emb,\n",
    "    \"norm_clips\": args.norm_clips,\n",
    "    \"norm_embed\": args.norm_embed,\n",
    "    \"token_mlp\": args.token_mlp,\n",
    "    }\n",
    "model = e3gnn_eclouds_clip_e2e(**kwargs)\n",
    "clip_computer = clip_loss_module()\n",
    "h_e3gnn, h_xformer, logits, bad_rows = model.forward_dist(\n",
    "    eclouds.to(device),\n",
    "    augmented_tokens.to(device),\n",
    "    atomic_number.to(device),\n",
    "    coords.to(device),\n",
    "    tokenizer,\n",
    "    p_clip_emb_smi=args.p_clip_emb_smi,\n",
    ")\n",
    "print('h_e3gnn', h_e3gnn.size())\n",
    "print('h_xformer', h_xformer.size())\n",
    "print('logits', logits.size())\n",
    "print('bad_rows', bad_rows.size())\n",
    "\n",
    "#loss\n",
    "bad_rows = bad_rows\n",
    "all_h_xformer = h_xformer\n",
    "all_h_e3gnn = h_e3gnn\n",
    "y_next = torch.zeros_like(augmented_tokens).to(device)\n",
    "y_next[:, :(augmented_tokens.shape[1] - 1)] = \\\n",
    "    torch.Tensor(augmented_tokens).clone()[:, 1:]\n",
    "y_next[y_next == tokenizer.clip_token] = -1\n",
    "y_next[y_next == tokenizer.pad_token] = -1\n",
    "y_next[y_next == tokenizer.smiles_token] = -1\n",
    "y_next[y_next == tokenizer.unk_token] = -1\n",
    "y_next[y_next == tokenizer.suffix_token] = -1\n",
    "y_next[y_next == tokenizer.middle_token] = -1\n",
    "\n",
    "ar_loss_ = torch.nn.functional.cross_entropy(\n",
    "                logits.view(-1, logits.size(-1)),\n",
    "                y_next.view(-1).long(),\n",
    "                ignore_index=-1,\n",
    "            )\n",
    "ar_loss = ar_loss_.mean()\n",
    "\n",
    "if args.do_clip:\n",
    "    clip_loss_ = clip_computer(all_h_xformer, all_h_e3gnn, bad_rows) \n",
    "    # clip_loss_ = clip_computer(h_xformer, h_e3gnn, bad_rows) \n",
    "    clip_loss = clip_loss_.mean()\n",
    "    loss = ar_loss + clip_loss * token_entropy_unit\n",
    "else:\n",
    "    loss = ar_loss\n",
    "\n",
    "print('loss', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecloudgen1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
