{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from model_ckpts/ecloud_smiles_67.pkl\n",
      "Loading tokenizer mar from model_ckpts/ecloud_smiles_67.pkl\n",
      "number of parameters: 12.64M\n",
      "number of parameters Total: 2.44M xformer: 19.60M Total: 22.04M \n",
      "Freezing encoder\n",
      "44882816 params frozen!\n"
     ]
    }
   ],
   "source": [
    "#define ecloud latent encoder\n",
    "import argparse\n",
    "import torch\n",
    "from coati.models.io.coati import load_e3gnn_smiles_clip_e2e\n",
    "from coati.models.regression.basic_due import basic_due\n",
    "from coati.utils.chem import read_sdf, write_sdf, rm_radical, sa, qed, logp\n",
    "from rdkit import Chem\n",
    "import random\n",
    "from coati.generative.molopt import gradient_opt\n",
    "from coati.generative.coati_purifications import embed_smiles\n",
    "from functools import partial\n",
    "from torch.nn.functional import sigmoid\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from coati.generative.coati_purifications import force_decode_valid_batch, embed_smiles, force_decode_valid\n",
    "import os.path as osp\n",
    "from coati.optimize.scoring import ScoringFunction\n",
    "from coati.optimize.mol_functions import qed_score, substructure_match_score, penalize_macrocycles, heavy_atom_count, penalized_logp_score\n",
    "from coati.optimize.pso_optimizer import BasePSOptimizer\n",
    "from coati.optimize.swarm import Swarm\n",
    "from coati.optimize.rules.qsar_score import qsar_model\n",
    "\n",
    "\n",
    "arg_parser = argparse.ArgumentParser(description='molecular optimization on the chemical space')\n",
    "arg_parser.add_argument('--device', choices=['cuda:0', 'cpu'], \\\n",
    "    default='cuda:3',help='Device')\n",
    "arg_parser.add_argument('--seed', type=int, default=2024) \n",
    "arg_parser.add_argument('--ecloudgen_ckpt', type=str, default = 'model_ckpts/ecloud_smiles_67.pkl')\n",
    "arg_parser.add_argument('--noise', type=float, default=0.3)\n",
    "args = arg_parser.parse_args([])\n",
    "\n",
    "# model loading\n",
    "DEVICE = torch.device(args.device)\n",
    "DEVICE = 'cuda:3'\n",
    "encoder, tokenizer = load_e3gnn_smiles_clip_e2e(\n",
    "    freeze=True,\n",
    "    device=DEVICE,\n",
    "    # model parameters to load.\n",
    "    doc_url=args.ecloudgen_ckpt,\n",
    ")\n",
    "\n",
    "#ecloud latent encoder\n",
    "class PSO_format_model():\n",
    "    def __init__(self, model, tokenizer, device):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "\n",
    "    def seq_to_emb(self, smiles):\n",
    "        if isinstance(smiles, str):\n",
    "            smi_emb = embed_smiles(smiles, self.model, self.tokenizer).to(DEVICE)\n",
    "            return smi_emb\n",
    "        else:\n",
    "            emb_list = []\n",
    "            for smi in smiles:\n",
    "                smi_emb = embed_smiles(smi, self.model, self.tokenizer).to(DEVICE)\n",
    "                emb_list.append(smi_emb)\n",
    "            return torch.stack(emb_list).reshape(-1, 256)\n",
    "\n",
    "    \n",
    "    def emb_to_seq(self, embs):\n",
    "\n",
    "        seq_list = []\n",
    "        for emb in embs:\n",
    "            seq = force_decode_valid_batch(emb, self.model, self.tokenizer)\n",
    "            seq_list.append(seq)\n",
    "        return seq_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.0585e-01,  1.0529e-01, -3.5812e-01,  2.7124e-02,  2.7386e-01,\n",
      "          8.6519e-02,  1.1036e-01, -1.0451e-02, -1.8966e-01, -2.5554e-01,\n",
      "         -3.3033e-01,  5.9155e-02, -1.6967e-02,  5.2427e-02, -9.2881e-02,\n",
      "          2.6020e-02,  1.4271e-01,  2.7295e-02, -8.7557e-02,  4.2929e-02,\n",
      "         -2.3445e-03, -1.7832e-01,  2.4162e-01, -3.1961e-01, -1.9733e-01,\n",
      "         -8.7914e-02, -1.8385e-01,  7.3459e-02, -5.1598e-01, -1.0449e-01,\n",
      "          3.2603e-01,  1.1386e+00,  1.6684e-01, -1.1404e-01, -1.5266e-01,\n",
      "         -2.2697e-01,  1.8601e-01, -2.7853e-02, -2.5724e-01,  1.4475e-01,\n",
      "         -1.4781e-02, -7.8996e-02, -7.8704e-03,  9.3806e-02, -2.3397e-01,\n",
      "          6.3077e-02, -1.4422e-01,  4.2236e-01,  9.7027e-03, -1.9467e-01,\n",
      "          1.8115e-01,  2.4048e-01,  8.8585e-02, -1.6303e-01, -1.6620e-01,\n",
      "          2.3039e-01, -5.4375e-02,  6.1500e-02, -1.3833e-01, -7.0604e-02,\n",
      "          1.1823e-01,  1.4608e-01, -2.3003e-01,  1.3378e-01,  2.7105e-02,\n",
      "          5.0183e-02,  6.9273e-02,  1.2783e-01, -1.9373e-01, -3.7547e-01,\n",
      "          2.5056e-01, -8.4696e-02, -1.3901e-01, -1.4646e-01, -7.8619e-02,\n",
      "          2.3794e-01,  1.5340e-01, -6.3299e-02,  2.4287e-02, -1.5243e-01,\n",
      "         -1.4532e-01, -1.4279e-01, -2.3779e-01, -2.6313e-01, -4.1251e-02,\n",
      "          7.6076e-02, -7.4574e-02,  8.4431e-02, -1.6998e-02,  1.6468e-02,\n",
      "         -1.1598e-01,  3.9145e-01, -8.2483e-02, -1.6703e-01,  5.5990e-02,\n",
      "         -1.2416e-01, -3.5268e-01,  3.4570e-01, -7.4254e-02, -1.0539e-01,\n",
      "         -2.8766e-01,  2.8723e-01, -2.1007e-02, -2.1229e-01,  4.6352e-01,\n",
      "         -7.6629e-02,  3.6840e-02,  3.2852e-02,  6.0734e-02,  1.1928e-01,\n",
      "          3.3035e-01, -7.4402e-03, -3.9584e-02,  2.3250e-01, -1.5175e-02,\n",
      "          1.8464e-01,  1.5652e-02, -5.5159e-03, -2.5020e-01,  1.1046e-01,\n",
      "         -3.6835e-01,  6.2479e-03, -3.5895e-01, -1.8127e-01,  1.4402e-01,\n",
      "          6.9179e-02,  1.3635e-01,  3.1167e-02, -2.1967e-01, -1.7517e-01,\n",
      "         -7.0074e-02,  1.4114e-02, -2.9079e-01,  2.7755e-01, -2.3818e-01,\n",
      "          1.2443e-01,  2.6293e-01, -2.9111e-01,  2.3556e-01, -2.6688e-01,\n",
      "          6.7204e-02,  2.0835e-01, -6.9733e-03,  2.5314e-01,  9.5276e-02,\n",
      "          1.1943e-01,  1.0161e-01,  5.5339e-02, -4.4774e-01, -2.1817e-01,\n",
      "         -2.6614e-01,  1.8551e-01, -2.3997e-02, -5.1874e-01,  2.0142e-01,\n",
      "         -8.7449e-01, -8.4431e-02,  2.0611e-01,  1.5030e-01, -2.4366e-01,\n",
      "          5.9837e-01,  3.3840e-01, -3.7218e-02, -3.0044e-04, -1.4956e-01,\n",
      "         -2.5091e-02, -6.0978e-02, -1.4850e-01,  1.7300e-01,  4.2265e-02,\n",
      "         -1.4377e-01, -3.7057e-02, -3.4607e-01,  6.1792e-03,  1.2085e-01,\n",
      "         -1.3883e-02, -1.3034e-01,  3.2358e-01, -5.4979e-01,  3.5070e-02,\n",
      "         -3.8154e-01,  7.6314e-02,  2.8465e-01,  1.9959e-02, -3.7329e-01,\n",
      "          6.8114e-02,  2.7184e-01, -4.0689e-01,  3.7693e-02,  3.0328e-01,\n",
      "          1.7147e-01, -2.1702e-02, -1.3993e-01, -1.2628e-02,  1.0343e-01,\n",
      "         -4.4309e-02,  3.9326e-01, -5.5678e-02,  1.5507e-01, -1.9313e-02,\n",
      "         -2.9986e-01,  9.1350e-02,  2.3246e-02,  6.8651e-02, -3.9089e-02,\n",
      "         -1.7691e-01,  3.5214e-01, -9.8578e-02, -2.3314e-01, -2.0968e-02,\n",
      "         -3.2282e-02, -1.5465e-01,  7.7138e-02, -4.2719e-02, -1.2482e-01,\n",
      "         -2.5698e-01,  6.8573e-02, -8.9025e-02,  2.9955e-01,  4.2016e-02,\n",
      "         -1.3645e-01,  8.7444e-02,  2.8878e-01,  9.1752e-02, -2.3287e-02,\n",
      "          1.7171e-01,  3.3553e-02,  1.9388e-01, -4.1489e-02, -5.0608e-01,\n",
      "         -5.3547e-02,  4.7703e-01, -4.4799e-02,  8.1160e-02, -4.6511e-01,\n",
      "         -1.2857e-02,  1.0871e-02, -2.3401e-01, -1.7376e-01, -2.0593e-02,\n",
      "         -8.0482e-02, -3.5722e-02,  4.7377e-02, -2.1575e-01,  1.2560e-02,\n",
      "         -3.9779e-02, -9.3099e-02, -1.0936e-01,  5.5464e-02,  2.9072e-01,\n",
      "         -8.4971e-02, -2.1824e-01,  3.8817e-02, -1.9907e-01, -3.1347e-01,\n",
      "         -5.0352e-02],\n",
      "        [-2.0585e-01,  1.0529e-01, -3.5812e-01,  2.7124e-02,  2.7386e-01,\n",
      "          8.6519e-02,  1.1036e-01, -1.0451e-02, -1.8966e-01, -2.5554e-01,\n",
      "         -3.3033e-01,  5.9155e-02, -1.6967e-02,  5.2427e-02, -9.2881e-02,\n",
      "          2.6020e-02,  1.4271e-01,  2.7295e-02, -8.7557e-02,  4.2929e-02,\n",
      "         -2.3445e-03, -1.7832e-01,  2.4162e-01, -3.1961e-01, -1.9733e-01,\n",
      "         -8.7914e-02, -1.8385e-01,  7.3459e-02, -5.1598e-01, -1.0449e-01,\n",
      "          3.2603e-01,  1.1386e+00,  1.6684e-01, -1.1404e-01, -1.5266e-01,\n",
      "         -2.2697e-01,  1.8601e-01, -2.7853e-02, -2.5724e-01,  1.4475e-01,\n",
      "         -1.4781e-02, -7.8996e-02, -7.8704e-03,  9.3806e-02, -2.3397e-01,\n",
      "          6.3077e-02, -1.4422e-01,  4.2236e-01,  9.7027e-03, -1.9467e-01,\n",
      "          1.8115e-01,  2.4048e-01,  8.8585e-02, -1.6303e-01, -1.6620e-01,\n",
      "          2.3039e-01, -5.4375e-02,  6.1500e-02, -1.3833e-01, -7.0604e-02,\n",
      "          1.1823e-01,  1.4608e-01, -2.3003e-01,  1.3378e-01,  2.7105e-02,\n",
      "          5.0183e-02,  6.9273e-02,  1.2783e-01, -1.9373e-01, -3.7547e-01,\n",
      "          2.5056e-01, -8.4696e-02, -1.3901e-01, -1.4646e-01, -7.8619e-02,\n",
      "          2.3794e-01,  1.5340e-01, -6.3299e-02,  2.4287e-02, -1.5243e-01,\n",
      "         -1.4532e-01, -1.4279e-01, -2.3779e-01, -2.6313e-01, -4.1251e-02,\n",
      "          7.6076e-02, -7.4574e-02,  8.4431e-02, -1.6998e-02,  1.6468e-02,\n",
      "         -1.1598e-01,  3.9145e-01, -8.2483e-02, -1.6703e-01,  5.5990e-02,\n",
      "         -1.2416e-01, -3.5268e-01,  3.4570e-01, -7.4254e-02, -1.0539e-01,\n",
      "         -2.8766e-01,  2.8723e-01, -2.1007e-02, -2.1229e-01,  4.6352e-01,\n",
      "         -7.6629e-02,  3.6840e-02,  3.2852e-02,  6.0734e-02,  1.1928e-01,\n",
      "          3.3035e-01, -7.4402e-03, -3.9584e-02,  2.3250e-01, -1.5175e-02,\n",
      "          1.8464e-01,  1.5652e-02, -5.5159e-03, -2.5020e-01,  1.1046e-01,\n",
      "         -3.6835e-01,  6.2479e-03, -3.5895e-01, -1.8127e-01,  1.4402e-01,\n",
      "          6.9179e-02,  1.3635e-01,  3.1167e-02, -2.1967e-01, -1.7517e-01,\n",
      "         -7.0074e-02,  1.4114e-02, -2.9079e-01,  2.7755e-01, -2.3818e-01,\n",
      "          1.2443e-01,  2.6293e-01, -2.9111e-01,  2.3556e-01, -2.6688e-01,\n",
      "          6.7204e-02,  2.0835e-01, -6.9733e-03,  2.5314e-01,  9.5276e-02,\n",
      "          1.1943e-01,  1.0161e-01,  5.5339e-02, -4.4774e-01, -2.1817e-01,\n",
      "         -2.6614e-01,  1.8551e-01, -2.3997e-02, -5.1874e-01,  2.0142e-01,\n",
      "         -8.7449e-01, -8.4431e-02,  2.0611e-01,  1.5030e-01, -2.4366e-01,\n",
      "          5.9837e-01,  3.3840e-01, -3.7218e-02, -3.0044e-04, -1.4956e-01,\n",
      "         -2.5091e-02, -6.0978e-02, -1.4850e-01,  1.7300e-01,  4.2265e-02,\n",
      "         -1.4377e-01, -3.7057e-02, -3.4607e-01,  6.1792e-03,  1.2085e-01,\n",
      "         -1.3883e-02, -1.3034e-01,  3.2358e-01, -5.4979e-01,  3.5070e-02,\n",
      "         -3.8154e-01,  7.6314e-02,  2.8465e-01,  1.9959e-02, -3.7329e-01,\n",
      "          6.8114e-02,  2.7184e-01, -4.0689e-01,  3.7693e-02,  3.0328e-01,\n",
      "          1.7147e-01, -2.1702e-02, -1.3993e-01, -1.2628e-02,  1.0343e-01,\n",
      "         -4.4309e-02,  3.9326e-01, -5.5678e-02,  1.5507e-01, -1.9313e-02,\n",
      "         -2.9986e-01,  9.1350e-02,  2.3246e-02,  6.8651e-02, -3.9089e-02,\n",
      "         -1.7691e-01,  3.5214e-01, -9.8578e-02, -2.3314e-01, -2.0968e-02,\n",
      "         -3.2282e-02, -1.5465e-01,  7.7138e-02, -4.2719e-02, -1.2482e-01,\n",
      "         -2.5698e-01,  6.8573e-02, -8.9025e-02,  2.9955e-01,  4.2016e-02,\n",
      "         -1.3645e-01,  8.7444e-02,  2.8878e-01,  9.1752e-02, -2.3287e-02,\n",
      "          1.7171e-01,  3.3553e-02,  1.9388e-01, -4.1489e-02, -5.0608e-01,\n",
      "         -5.3547e-02,  4.7703e-01, -4.4799e-02,  8.1160e-02, -4.6511e-01,\n",
      "         -1.2857e-02,  1.0871e-02, -2.3401e-01, -1.7376e-01, -2.0593e-02,\n",
      "         -8.0482e-02, -3.5722e-02,  4.7377e-02, -2.1575e-01,  1.2560e-02,\n",
      "         -3.9779e-02, -9.3099e-02, -1.0936e-01,  5.5464e-02,  2.9072e-01,\n",
      "         -8.4971e-02, -2.1824e-01,  3.8817e-02, -1.9907e-01, -3.1347e-01,\n",
      "         -5.0352e-02]], device='cuda:3')\n",
      "torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "#encoder example\n",
    "ecloud_latent = PSO_format_model(encoder, tokenizer, DEVICE)\n",
    "init_smiles = \"c1ccccc1\"\n",
    "init_emb = ecloud_latent.seq_to_emb([init_smiles, init_smiles])\n",
    "print(init_emb)\n",
    "print(init_emb.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C[C@@H]1[C@]2(C)[CH+]C[C@]13CC[C@](C)(CCC2)C3', 'CC(C)[C@@H]1C[C@H]1[C+]1[C@H]2C[C@@]2(C)CC[C@@H]1C', 'C=C1CC[C@H]([C@H](C)[C@H]2C[CH+]C[C@H](C)C2)C1', 'C=C1CC[C@H]2C[C@@H]1CCC[C+](C)C[C@@H]2C', 'C=C(C[C@]1(C)C[C@@H]1C)[C+]1CCC(C)CC1', 'C[CH+]CC1=C[C@H](C)[C@@]12CC[C@@H](C(C)C)C2', 'C[C@@H]1CC[C@@H]1[C+]1CC[C@H](C)[C@H]2CC[C@@]12C', 'C=C(C)[C@H]1CC[C@@]2(C1)[C+](C)CCC[C@@H]2C', 'C[C@H]1C[C@H](C)C12C[C@@H]1CC[C+]1CC2(C)C', 'CC(C)[C@]12C[C@@]3(C[CH+][C@@H]1C)[C@@H](C)CC[C@H]32', 'C[C+]1C[C@H](C2=C[C@H](C)CCC2)CC[C@@H]1C', 'C[C+]1C[C@]23[C@@H](C)C[C@H](C[C@H]2C1(C)C)[C@H]3C', 'C=C1[C+](CC(C)C)C[C@@H](C)[C@@H]2C[C@@H]2[C@H]1C', 'CC(C)=C1CCC[C+](C)[C@@H]2CC[C@H](C)[C@@H]12', 'C[C@@H]1[CH+][C@@H]2CC/C2=C\\\\CC(C)(C)CCC1', 'CC(C)=C[CH+]C/C1=C/CC[C@@H](C)CCC1', 'C[C@@H]1[C@@H]2C[C@@H]([C@@H]3[CH+]C(C)(C)CCC3)[C@H]1C2', 'CC1=C[C@H]2[C+](C(C)C)CC[C@@]2(C)CCC1', 'C=C[C+]1[C@H](C)CC[C@]1(C)CCC=C(C)C', 'CC(C)[C@@H]1CC[C@H](C)[C@@]12[CH+][C@]1(C[C@@H]1C)C2', 'C[C+]1CCCC(C)(C)CC[C@@]12C[C@H]1C[C@H]12', 'C[C+]1C[C@@H]2[C@@H](CC(C)C)[C@H]1C=CC2(C)C', 'C/C1=C/C[C+](C)C/C=C\\\\C[C@H](C)CCC1', 'C/C1=C\\\\[C@@H](C)/C=C\\\\C(C)(C)CCC[C+]1C', 'C=CC(C)(C)[C@H]1CC[C+]2CC(C)(C)[C@@]21C', 'C[C@H]1CC[C@H](C)[C@@H]2[C@@H]3[C+]1CC[C@@H]3C2(C)C', 'C[C@H]1C=C[CH+][C@H]([C@]2(C)CCCC2(C)C)C1', 'C[C+]1CC[C@]2(C)CCC[C@@H]([C@@H]3C[C@@H]32)[C@H]1C', 'CC(C)=C1CC[C@H](C)[C+]2CC[C@@H](C)[C@H]2C1', 'C=C1CCC[C+](C)[C@@H]2C[C@@H](C)[C@@H](C)[C@H]2C1', 'C[C+]1C[C@@H](C)[C@H]([C@@H]2C[C@@H]2C(C)C)[C@@H]2C[C@H]12', 'C[C+]1C/C(=C/[C@H]2C[C@H]2C(C)C)CC[C@H]1C', 'CC1=C2CC[C@H](C)C[C@H]2[C@@H]([C+](C)C)CC1', 'C/C1=C\\\\CC/C(C)=C/C[C@@H](C)[C@H](C)C[CH+]1', 'CC(C)=C1C[C@@H]1[C@H](C)[C+]1C[C@H](C)C[C@@H]1C', 'C/C1=C\\\\C[C@@H](C)[C+](C)/C=C/[C@H](C)CCC1', 'C[C@@H]1[CH+]C[C@@H]2[C@H]1C[C@@H]1CC(C)(C)CC[C@@H]12', 'C=C(C)[C@H]1CCC[C@H]2CC[C@]2(C)[CH+][C@@H]1C', 'C[C+]1C/C2=C/CC[C@H](C)CC[C@@H](C)[C@H]1C2', 'CC(C)=CCC[C@@H](C)C[C@@H]1[CH+]C=C[C@@H]1C', 'C[C@H]1CC=C[C@H](C)[C@@H]2[CH+]CC(C)(C)C[C@@H]12', 'C=C(C)[C@H]1C[C+](C)[C@@H]2C[C@@H]2CC[C@H](C)C1', 'CC1=C2CCC(C)(C)C[C@]2(C)C[CH+]CC1', 'C[CH+][C@H]1CC[C@@H]1[C@]1(C)CC[C@@H](C)[C@@H]2C[C@@H]21', 'C=C1CC[C@]2(C)CCC[C@H](C)C[C+]1CC2', 'C[C+]1[C@H](C)[C@@H]2C[C@@]3(C)[C@@H](C)CC[C@@]3(C)[C@H]12', 'CC(C)=C1CC[C+]2[C@@H](C)CC[C@H](C)[C@@H]2C1', 'CC1(C)CCC[C@@]1(C)[C@H]1C[C@H]2[CH+]CC[C@H]21', 'C=CCC(=C(C)C)[C@H](C)[C@@H]1[CH+][C@@H](C)CC1', 'C[C+](C)[C@@]12CC[C@@H](C)[C@@H](CC1)[C@@]1(C)C[C@@H]21', 'C[C+]1[C@H]2C[C@H](C(C)C)C[C@H]2[C@@H](C)[C@@H]2C[C@H]12', 'C=C1CC[C@@H]2[C@@H]1C[C@H](C(C)C)C[CH+][C@H]2C', 'C=C1CCC2(CC1)CC[C+](C)[C@@H](C)[C@H]2C', 'C[C+]1[C@H]2C[C@H]2[C@]2(C)CC[C@H](C(C)C)C[C@H]12', 'C/C1=C2\\\\C[C@@H]2CC[C+](C)[C@H](C(C)C)CC1', 'C=C(C)C[C@H]1C=C([CH2+])C(C)(C)CC[C@H]1C', 'C/C1=C\\\\CC[C+](C)[C@@H]2[C@@H](C(C)C)[C@@H]2CC1', 'C[C+]1CC[C@@H](C)C/C=C/[C@H]2C[C@H](C2)[C@H]1C', 'C[C+]1C/C=C/C(C)(C)C[C@@H]2C[C@@H](C)C[C@H]12', 'C[C+]1CC[C@@H]2[C@H](C)/C=C\\\\CC(C)(C)C[C@@H]12', 'C=C1CCC[C@H]1[C@H](C)[C@H]1CCC[C+](C)C1', 'C=C1CC[C+]([C@H](C)C[C@H]2CCC[C@H]2C)C1', 'C=C1C[C@H](C)[CH+]C[C@@H](C(=C)C)CC[C@@H]1C', 'CC1=CC[C@H](C)[C+]2CC[C@H](C(C)C)C[C@H]12']\n",
      "tensor([ -3.5419, -32.3167,  -9.9739, -21.1855, -16.3648,  -5.1258, -17.0976,\n",
      "        -29.3679,  -0.6482,  -7.3472, -21.0357, -37.2951, -13.6573, -27.7165,\n",
      "         -5.2472, -23.1680, -11.6898, -33.7773, -16.1548,  -3.7326,  -0.2358,\n",
      "        -38.5395, -14.9660, -12.2269, -16.4689, -33.9242, -27.7585,  -6.5351,\n",
      "        -30.3557, -27.6717, -16.8552, -23.7588, -31.3262,   3.2041,  -6.3956,\n",
      "        -18.6381, -29.2069, -20.7447, -19.6443, -14.2677, -11.8968,  -4.1202,\n",
      "        -11.1060,   0.1980, -25.5419, -17.2992, -36.0158,  -9.7620, -34.9955,\n",
      "        -25.3754, -30.0968, -15.8941, -30.1516, -35.4899,   3.9113,  -4.7021,\n",
      "        -20.8971,  -2.5477, -28.0484, -23.0739, -21.8258, -23.9077,   1.7938,\n",
      "        -35.1370])\n"
     ]
    }
   ],
   "source": [
    "#data loading\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "# 自定义 Dataset 类\n",
    "class PaddleDataset(Dataset):\n",
    "    def __init__(self, path, label_name=None):\n",
    "        # 读取 CSV 文件\n",
    "        self.df = pd.read_csv(path)\n",
    "        # 提取 smiles 列\n",
    "        self.smiles = self.df['smiles'].values\n",
    "        # 如果有标签列，则提取标签\n",
    "        self.labels = None\n",
    "        if label_name is not None:\n",
    "            self.labels = self.df[label_name].values\n",
    "\n",
    "    def __len__(self):\n",
    "        # 数据集的大小\n",
    "        return len(self.smiles)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 根据索引返回单个样本\n",
    "        sample = {'smiles': self.smiles[idx]}\n",
    "        if self.labels is not None:\n",
    "            sample['label'] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return sample\n",
    "\n",
    "# 创建 DataLoader 的函数\n",
    "from sklearn.model_selection import train_test_split\n",
    "def split_dataloader(csv_path, label_name=None, batch_size = 64, eval_batch_size = 64, shuffle=True, num_workers=4):\n",
    "    \n",
    "    dataset = PaddleDataset(csv_path, label_name)\n",
    "    train, valid = train_test_split(dataset, random_state=42, test_size=0.2)\n",
    "    trn_loader = DataLoader(train, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)\n",
    "    valid_loader = DataLoader(valid, batch_size=eval_batch_size, shuffle=shuffle, num_workers=num_workers)\n",
    "    return trn_loader, valid_loader\n",
    "\n",
    "def get_tst_dataloader(csv_path, label_name=None, batch_size = 64, eval_batch_size = 64, shuffle=True, num_workers=4):\n",
    "    \n",
    "    dataset = PaddleDataset(csv_path, label_name)\n",
    "    dataloader = DataLoader(dataset, batch_size=eval_batch_size, shuffle=shuffle, num_workers=num_workers)\n",
    "    return dataloader\n",
    "# 使用示例\n",
    "trian_csv_path = 'data/train.csv'\n",
    "trn_loader, valid_loader = split_dataloader(trian_csv_path, label_name='label')\n",
    "test_csv_path = 'data/test_nopoint.csv'\n",
    "tst_loader = get_tst_dataloader(test_csv_path, label_name=None)\n",
    "\n",
    "\n",
    "# 查看 DataLoader 的样本\n",
    "for batch in trn_loader:\n",
    "    print(batch['smiles'])\n",
    "    print(batch['label'])\n",
    "    break  # 这里只是查看第一个批次的数据\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'smiles': ['C[C@@H]1CC[C@H](C[C+]2CC[C@H](C)[C@@H]3C[C@H]23)C1', 'CC1=C[C@H]2C[C@@H]1C[C@H](C(C)C)[CH+]C[C@H]2C', 'C[C+]1CC[C@H]2[C@@H](C)CCCC[C@@]23CC[C@@H]13', 'C=C(C)[C@@H]1C=CC[C@@H]([C+](C)CC)C1(C)C', 'CC(C)=C[CH+][C@@H]1C/C=C/CC[C@@H](C)[C@@H]1C', 'C[C+](C)/C1=C/[C@@H]2[C@@H](C)C[C@H]2C[C@@H](C)CC1', 'C/C1=C/C[C+]2C[C@@H](C)[C@@H](CC1)CC[C@@H]2C', 'C=C1C[C@H](C)[C@@H]2C[C@@]2(C)[C@@H](C)CC[C+]1C', 'C/C=C\\\\CCCC1=C[C+](C)CCC[C@H]1C', 'C[C@H]1CC[C+]2CCC[C@]3(C)CC[C@@]2(C1)C3', 'C=C1C[CH+]C[C@@]12CCC[C@@H](C)CC[C@H]2C', 'C[C@H]1C[CH+][C@@H]2[C@H](CC1)[C@]21CCCC1(C)C', 'C[C+]1CC=C[C@](C)(C[C@H]2C[C@H](C)C2)CC1', 'C=C1CCC2(CC1)CC[C+](C)CC2(C)C', 'CC1(C)C[CH+][C@@](C)(CC=C2CC2)CCC1', 'C[C@H]1[CH+][C@H]2[C@@H](CC1)[C@H](C)[C@@H]1[C@@H](C)[C@H](C)[C@H]21', 'C[C@H]1CC[C@@H]2C[C@H]3[C@H](C)[CH+]C[C@@H]1[C@@H]3[C@H]2C', 'C[C@H]1CCC[C@@H]2C[C@@H]3[C+]2[C@@H](CC[C@@H]3C)C1', 'C[C@H]1C(C)(C)[C@@H]2[CH+]CC[C@@](C)(C2)C12CC2', 'CC(C)[C@H]1CC[C+]2[C@H](C1)[C@@H](C)C=C[C@@H]2C', 'C[C@H]1CCC[C@@H]2C[C+]1[C@]1(C)CC[C@H]2CC1', 'CC(C)[C@H]1[C@H]2C[C@@H]3C[C@H]2[C@@H](C)[C@H]1[CH+][C@@H]3C', 'CC(C)[C@H]1CC[C@H](C)[C+]1C[C@@]12C[C@@H]1[C@@H]2C', 'C[C@H]1C[CH+]C[C@]23C[C@H]2CCC(C)(C)C[C@H]13', 'C[C@@H]1CC=C1[CH+]C[C@]1(C)CCCC1(C)C', 'C[C+]1C[C@]2(C)CC[C@H]1[C@H](C)[C@@]21CC[C@@H]1C', 'CC[C@@H]1C[C+]([C@@H](C)[C@H]2CC2=C(C)C)[C@@H]1C', 'C[CH+][C@@]12CC[C@@H]3[C@@H]1[C@](C)(CC2)CC3(C)C', 'C[C+](C)C1=CC[C@H](C)[C@@H]2CC[C@H](C)[C@@H]2C1', 'CC(C)/C(=C1\\\\C[CH+][C@H](C)CC1)C1(C)CC1', 'C[C@H]1CCC[C@H]2[C+](CC2(C)C)C12CCC2', 'C[C+]1C[C@@H]2[C@H](C)CC[C@H](C)[C@@]23CC[C@@H]1C3', 'C=C1[C@@H](CC[C+](C)C)[C@@H]2[C@@H](C)CCC[C@@H]12', 'CC(C)[C@H]1CC[C@]2(C)CC[C@@]3(C[C@@H]3C)[C+]12', 'C=C1CCC[C@@]2(C)C[C@@H]1C(C)(C)C[C+]2C', 'C=C1[C@H]2C[C@@H](C[C@H]1CC[CH+]C(C)C)[C@H]2C', 'C=C(C)C[C@H]1[CH+][C@@H](C)C[C@H]2C[C@@H](C)C[C@@H]21', 'CC1=CCC/C(=C(\\\\C)[CH+]CCC(C)C)C1', 'CC(C)[C@H]1C[C@@H]1[C+]1[C@@H](C)C[C@H](C)C12CC2', 'C[C+]1CC[C@@H]2[C@@H](C)CCC(C(C)C)=C[C@H]12', 'C=CC[C+](C)[C@H]1CC(C)(C)[C@H]2C[C@H]2[C@H]1C', 'C[C+]1CC[C@H](C(C)C)[C@H]2[C@H]1[C@H]1CC[C@@]21C', 'C=C1C/C(=C/C)[CH+]CC[C@H](C)CC[C@@H]1C', 'CC1=C/CC[C@@H](C)C/C(C[CH+]C(C)C)=C\\\\1', 'C[C@@H]1CC[C@@H]2[C@H]3[C@H]1[CH+]C[C@]3(C)CC2(C)C', 'C[C@H]1CC[C+]2CCC[C@H]3CC[C@@]1(C)[C@@H]3C2', 'C[C+]1CCC[C@H]2C[C@@H]3C(C)(C)C[C@]23CC1', 'CC[C@@H](C)C/C=C1\\\\C=C[C@H](C)[CH+]C[C@H]1C', 'C[C@H]1CC[C+]2CC(C)(C)CCC[C@]23C[C@H]13', 'C=C1CCC[C+]1[C@@H](C)[C@H]1CC[C@H](C)CC1', 'C/C1=C\\\\C[CH+][C@H](C)C/C=C(/C(C)C)CC1', 'CC1CC[C+]([C@]2(C)CC=CC2(C)C)CC1', 'CC1(C)[CH+]CC[C@H]2C[C@H]3CC[C@@H](CC1)[C@H]32', 'C[C@@H]1[CH+]CC(C)(C)[C@@H]2C[C@]3(C)C[C@]1(C2)C3', 'C[C@@H]1[C+]2CC(C)(C2)C[C@H](C)[C@H]2CC[C@@H]1C2', 'C[C@H]1CCC[C@H](C)[C+]2C[C@H]3[C@@H]2[C@H]3[C@H](C)C1', 'C=C(C)CCCCC1=C(C)CC[C+](C)C1', 'C[C@@H]1[CH+]C[C@@]2(C)C(C)(C)C3CCC12CC3', 'C=C1[CH+][C@H](CCCC(C)(C)C)CCC1=C', 'CC[C@@H](C)[C@@H]1C[C+]2CC[C@H]3[C@@H]1[C@@]23C(C)C', 'CCC1=CC/C(=C(\\\\C)C[CH+]CC(C)C)C1', 'C=C1C[CH+][C@@H](C(C)C)CCC(=C)CCC1', 'CC(C)[C@@H]1CC[C@H](C)[C@H]2C=C([CH+]2)[C@@H](C)C1', 'C[C@H]1CC[C+]([C@@H]2C[C@@H]3C[C@H]2CC[C@H]3C)C1']}\n"
     ]
    }
   ],
   "source": [
    "#encoder example with dataloader\n",
    "ecloud_latent = PSO_format_model(encoder, tokenizer, DEVICE)\n",
    "init_smiles = \"c1ccccc1\"\n",
    "init_emb = ecloud_latent.seq_to_emb([init_smiles, init_smiles])\n",
    "# print(init_emb)\n",
    "# print(init_emb.size())\n",
    "# for batch in trn_loader:\n",
    "#     print(batch['smiles'])\n",
    "#     print(batch['label'])\n",
    "#     ecloud_latent = PSO_format_model(encoder, tokenizer, DEVICE)\n",
    "#     init_smils = batch['smiles']\n",
    "#     init_emb = ecloud_latent.seq_to_emb(init_smiles)\n",
    "#     print(init_emb)\n",
    "#     print(init_emb.size())\n",
    "#     break  # 这里只是查看第一个批次的数据\n",
    "\n",
    "for data in tst_loader:\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model module\n",
    "import torch.nn as nn\n",
    "\n",
    "class ecloud_regre(nn.Module):\n",
    "    def __init__(self, hidden_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = PSO_format_model(encoder, tokenizer, DEVICE)\n",
    "        self.pred = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_dim, hidden_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_dim, out_dim)\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, data):\n",
    "\n",
    "        smiles_out = self.encoder.seq_to_emb(data['smiles']) # (n, 256)\n",
    "\n",
    "        out = self.pred(smiles_out)\n",
    "\n",
    "        return out\n",
    "\n",
    "model = ecloud_regre(256, 1)\n",
    "# for data in trn_loader:\n",
    "#     model = model.to(DEVICE)\n",
    "#     out = model(data)\n",
    "#     print('out', out.size())\n",
    "#     print('label', data['label'])\n",
    "#     break\n",
    "y_pred = torch.tensor([]).to(DEVICE)\n",
    "for data in tst_loader:\n",
    "    model = model.to(DEVICE)\n",
    "    out = model(data)\n",
    "    y_pred = torch.cat((y_pred, out))\n",
    "    # print('out', out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, LR: 0.000500, Train MAE: 9.7103853, Train RMSE: 11.5048351, Valid MAE: 9.8144123, Valid RMSE: 11.5938349, Best Valid RMSE: 11.5938349, Best Epoch: 000, Time:0.550 min\n",
      "Epoch: 001, LR: 0.000500, Train MAE: 9.4157882, Train RMSE: 11.3306274, Valid MAE: 9.9230983, Valid RMSE: 11.7602301, Best Valid RMSE: 11.5938349, Best Epoch: 000, Time:0.557 min\n",
      "Epoch: 002, LR: 0.000500, Train MAE: 9.3884908, Train RMSE: 11.2604036, Valid MAE: 9.7890697, Valid RMSE: 11.5691833, Best Valid RMSE: 11.5691833, Best Epoch: 002, Time:0.529 min\n",
      "Epoch: 003, LR: 0.000500, Train MAE: 9.3486824, Train RMSE: 11.2022820, Valid MAE: 9.7755542, Valid RMSE: 11.5531330, Best Valid RMSE: 11.5531330, Best Epoch: 003, Time:0.540 min\n",
      "Epoch: 004, LR: 0.000500, Train MAE: 9.3277785, Train RMSE: 11.1984711, Valid MAE: 9.7768602, Valid RMSE: 11.5643015, Best Valid RMSE: 11.5531330, Best Epoch: 003, Time:0.536 min\n",
      "Epoch: 005, LR: 0.000500, Train MAE: 9.3010987, Train RMSE: 11.1538038, Valid MAE: 9.7498393, Valid RMSE: 11.5254536, Best Valid RMSE: 11.5254536, Best Epoch: 005, Time:0.579 min\n",
      "Epoch: 006, LR: 0.000500, Train MAE: 9.2759031, Train RMSE: 11.1431580, Valid MAE: 9.7371214, Valid RMSE: 11.5117617, Best Valid RMSE: 11.5117617, Best Epoch: 006, Time:0.536 min\n",
      "Epoch: 007, LR: 0.000500, Train MAE: 9.2567894, Train RMSE: 11.1196756, Valid MAE: 9.7123999, Valid RMSE: 11.4794874, Best Valid RMSE: 11.4794874, Best Epoch: 007, Time:0.545 min\n",
      "Epoch: 008, LR: 0.000500, Train MAE: 9.2548357, Train RMSE: 11.1250687, Valid MAE: 9.7091137, Valid RMSE: 11.4873333, Best Valid RMSE: 11.4794874, Best Epoch: 007, Time:0.539 min\n",
      "Epoch: 009, LR: 0.000500, Train MAE: 9.2319580, Train RMSE: 11.0550737, Valid MAE: 9.6728232, Valid RMSE: 11.4310274, Best Valid RMSE: 11.4310274, Best Epoch: 009, Time:0.558 min\n",
      "Epoch: 010, LR: 0.000500, Train MAE: 9.2083608, Train RMSE: 11.0529718, Valid MAE: 9.6929964, Valid RMSE: 11.4752064, Best Valid RMSE: 11.4310274, Best Epoch: 009, Time:0.616 min\n",
      "Epoch: 011, LR: 0.000500, Train MAE: 9.2079574, Train RMSE: 11.1094904, Valid MAE: 9.6542048, Valid RMSE: 11.4158707, Best Valid RMSE: 11.4158707, Best Epoch: 011, Time:0.575 min\n",
      "Epoch: 012, LR: 0.000500, Train MAE: 9.1958602, Train RMSE: 11.0285616, Valid MAE: 9.6196134, Valid RMSE: 11.3697348, Best Valid RMSE: 11.3697348, Best Epoch: 012, Time:0.549 min\n",
      "Epoch: 013, LR: 0.000500, Train MAE: 9.1769287, Train RMSE: 11.0209684, Valid MAE: 9.6421316, Valid RMSE: 11.4134979, Best Valid RMSE: 11.3697348, Best Epoch: 012, Time:0.536 min\n",
      "Epoch: 014, LR: 0.000500, Train MAE: 9.1702100, Train RMSE: 11.0375175, Valid MAE: 9.6403664, Valid RMSE: 11.4236917, Best Valid RMSE: 11.3697348, Best Epoch: 012, Time:0.535 min\n",
      "Epoch: 015, LR: 0.000500, Train MAE: 9.1469470, Train RMSE: 10.9938765, Valid MAE: 9.5877927, Valid RMSE: 11.3480024, Best Valid RMSE: 11.3480024, Best Epoch: 015, Time:0.511 min\n",
      "Epoch: 016, LR: 0.000500, Train MAE: 9.1475431, Train RMSE: 11.0052414, Valid MAE: 9.5963651, Valid RMSE: 11.3734074, Best Valid RMSE: 11.3480024, Best Epoch: 015, Time:0.522 min\n",
      "Epoch: 017, LR: 0.000500, Train MAE: 9.1350472, Train RMSE: 10.9555712, Valid MAE: 9.5984479, Valid RMSE: 11.3901482, Best Valid RMSE: 11.3480024, Best Epoch: 015, Time:0.545 min\n",
      "Epoch: 018, LR: 0.000500, Train MAE: 9.1190200, Train RMSE: 11.0169773, Valid MAE: 9.6135144, Valid RMSE: 11.4308376, Best Valid RMSE: 11.3480024, Best Epoch: 015, Time:0.534 min\n",
      "Epoch: 019, LR: 0.000500, Train MAE: 9.1153762, Train RMSE: 10.9634638, Valid MAE: 9.5678782, Valid RMSE: 11.3592606, Best Valid RMSE: 11.3480024, Best Epoch: 015, Time:0.557 min\n",
      "Epoch: 020, LR: 0.000500, Train MAE: 9.1034483, Train RMSE: 10.9928169, Valid MAE: 9.5891030, Valid RMSE: 11.4111462, Best Valid RMSE: 11.3480024, Best Epoch: 015, Time:0.543 min\n",
      "Epoch: 021, LR: 0.000500, Train MAE: 9.0911475, Train RMSE: 10.9438725, Valid MAE: 9.5630510, Valid RMSE: 11.3744717, Best Valid RMSE: 11.3480024, Best Epoch: 015, Time:0.548 min\n",
      "Epoch: 022, LR: 0.000500, Train MAE: 9.0820439, Train RMSE: 10.9562845, Valid MAE: 9.5894292, Valid RMSE: 11.4341297, Best Valid RMSE: 11.3480024, Best Epoch: 015, Time:0.557 min\n",
      "Epoch: 023, LR: 0.000500, Train MAE: 9.0651164, Train RMSE: 10.9557009, Valid MAE: 9.5261843, Valid RMSE: 11.3293486, Best Valid RMSE: 11.3293486, Best Epoch: 023, Time:0.574 min\n",
      "Epoch: 024, LR: 0.000500, Train MAE: 9.0595252, Train RMSE: 10.9119320, Valid MAE: 9.5473984, Valid RMSE: 11.3777533, Best Valid RMSE: 11.3293486, Best Epoch: 023, Time:0.517 min\n",
      "Epoch: 025, LR: 0.000500, Train MAE: 9.0583132, Train RMSE: 10.9606915, Valid MAE: 9.5012642, Valid RMSE: 11.3014688, Best Valid RMSE: 11.3014688, Best Epoch: 025, Time:0.532 min\n",
      "Epoch: 026, LR: 0.000500, Train MAE: 9.0393771, Train RMSE: 10.8949432, Valid MAE: 9.5545705, Valid RMSE: 11.4156590, Best Valid RMSE: 11.3014688, Best Epoch: 025, Time:0.560 min\n",
      "Epoch: 027, LR: 0.000500, Train MAE: 9.0415278, Train RMSE: 10.9547253, Valid MAE: 9.5484492, Valid RMSE: 11.4106541, Best Valid RMSE: 11.3014688, Best Epoch: 025, Time:0.561 min\n",
      "Epoch: 028, LR: 0.000500, Train MAE: 9.0488109, Train RMSE: 10.9308500, Valid MAE: 9.4624020, Valid RMSE: 11.2620907, Best Valid RMSE: 11.2620907, Best Epoch: 028, Time:0.532 min\n",
      "Epoch: 029, LR: 0.000500, Train MAE: 9.0083884, Train RMSE: 10.8999338, Valid MAE: 9.5511960, Valid RMSE: 11.4386110, Best Valid RMSE: 11.2620907, Best Epoch: 028, Time:0.513 min\n",
      "Epoch: 030, LR: 0.000500, Train MAE: 9.0218886, Train RMSE: 10.9765043, Valid MAE: 9.4624590, Valid RMSE: 11.2814512, Best Valid RMSE: 11.2620907, Best Epoch: 028, Time:0.519 min\n",
      "Epoch: 031, LR: 0.000500, Train MAE: 9.0100992, Train RMSE: 10.8747025, Valid MAE: 9.4889094, Valid RMSE: 11.3370714, Best Valid RMSE: 11.2620907, Best Epoch: 028, Time:0.527 min\n",
      "Epoch: 032, LR: 0.000500, Train MAE: 8.9958290, Train RMSE: 10.9133959, Valid MAE: 9.4699102, Valid RMSE: 11.3101397, Best Valid RMSE: 11.2620907, Best Epoch: 028, Time:0.525 min\n",
      "Epoch: 033, LR: 0.000500, Train MAE: 8.9890272, Train RMSE: 10.8747416, Valid MAE: 9.5349988, Valid RMSE: 11.4402552, Best Valid RMSE: 11.2620907, Best Epoch: 028, Time:0.531 min\n",
      "Epoch: 034, LR: 0.000500, Train MAE: 8.9952198, Train RMSE: 10.9400234, Valid MAE: 9.4437079, Valid RMSE: 11.2784672, Best Valid RMSE: 11.2620907, Best Epoch: 028, Time:0.554 min\n",
      "Epoch: 035, LR: 0.000500, Train MAE: 8.9773030, Train RMSE: 10.8923073, Valid MAE: 9.4859552, Valid RMSE: 11.3686533, Best Valid RMSE: 11.2620907, Best Epoch: 028, Time:0.540 min\n",
      "Epoch: 036, LR: 0.000500, Train MAE: 8.9765269, Train RMSE: 10.8910513, Valid MAE: 9.4504531, Valid RMSE: 11.3061657, Best Valid RMSE: 11.2620907, Best Epoch: 028, Time:0.544 min\n",
      "Epoch: 037, LR: 0.000500, Train MAE: 8.9664438, Train RMSE: 10.9141512, Valid MAE: 9.4619957, Valid RMSE: 11.3268394, Best Valid RMSE: 11.2620907, Best Epoch: 028, Time:0.537 min\n",
      "Epoch: 038, LR: 0.000500, Train MAE: 8.9607485, Train RMSE: 10.8619967, Valid MAE: 9.4471392, Valid RMSE: 11.3083391, Best Valid RMSE: 11.2620907, Best Epoch: 028, Time:0.512 min\n",
      "Epoch: 039, LR: 0.000500, Train MAE: 8.9569145, Train RMSE: 10.8717270, Valid MAE: 9.5016541, Valid RMSE: 11.4142561, Best Valid RMSE: 11.2620907, Best Epoch: 028, Time:0.521 min\n",
      "Epoch: 040, LR: 0.000500, Train MAE: 8.9540788, Train RMSE: 10.9224930, Valid MAE: 9.4156663, Valid RMSE: 11.2652864, Best Valid RMSE: 11.2620907, Best Epoch: 028, Time:0.524 min\n",
      "Epoch: 041, LR: 0.000500, Train MAE: 8.9873018, Train RMSE: 10.8316603, Valid MAE: 9.4462845, Valid RMSE: 11.3241091, Best Valid RMSE: 11.2620907, Best Epoch: 028, Time:0.525 min\n",
      "Epoch: 042, LR: 0.000500, Train MAE: 8.9545023, Train RMSE: 10.9624767, Valid MAE: 9.4322415, Valid RMSE: 11.3030939, Best Valid RMSE: 11.2620907, Best Epoch: 028, Time:0.542 min\n",
      "Epoch: 043, LR: 0.000500, Train MAE: 8.9427548, Train RMSE: 10.8153524, Valid MAE: 9.4282272, Valid RMSE: 11.3030691, Best Valid RMSE: 11.2620907, Best Epoch: 028, Time:0.521 min\n",
      "Epoch: 044, LR: 0.000500, Train MAE: 8.9279781, Train RMSE: 10.8857107, Valid MAE: 9.4603486, Valid RMSE: 11.3651657, Best Valid RMSE: 11.2620907, Best Epoch: 028, Time:0.525 min\n",
      "Epoch: 045, LR: 0.000500, Train MAE: 8.9219458, Train RMSE: 10.8437138, Valid MAE: 9.4107542, Valid RMSE: 11.2856321, Best Valid RMSE: 11.2620907, Best Epoch: 028, Time:0.505 min\n",
      "Epoch: 046, LR: 0.000500, Train MAE: 8.9248837, Train RMSE: 10.8775883, Valid MAE: 9.4383810, Valid RMSE: 11.3456697, Best Valid RMSE: 11.2620907, Best Epoch: 028, Time:0.504 min\n",
      "Epoch: 047, LR: 0.000500, Train MAE: 8.9138226, Train RMSE: 10.8417273, Valid MAE: 9.3886345, Valid RMSE: 11.2562742, Best Valid RMSE: 11.2562742, Best Epoch: 047, Time:0.532 min\n",
      "Epoch: 048, LR: 0.000500, Train MAE: 8.9075857, Train RMSE: 10.8000431, Valid MAE: 9.4450455, Valid RMSE: 11.3641167, Best Valid RMSE: 11.2562742, Best Epoch: 047, Time:0.536 min\n",
      "Epoch: 049, LR: 0.000500, Train MAE: 8.9169220, Train RMSE: 10.9229736, Valid MAE: 9.4080395, Valid RMSE: 11.3019867, Best Valid RMSE: 11.2562742, Best Epoch: 047, Time:0.535 min\n",
      "Epoch: 050, LR: 0.000500, Train MAE: 8.9010200, Train RMSE: 10.8478060, Valid MAE: 9.4255264, Valid RMSE: 11.3407421, Best Valid RMSE: 11.2562742, Best Epoch: 047, Time:0.541 min\n",
      "Epoch: 051, LR: 0.000500, Train MAE: 8.9014461, Train RMSE: 10.8249855, Valid MAE: 9.3744129, Valid RMSE: 11.2463284, Best Valid RMSE: 11.2463284, Best Epoch: 051, Time:0.538 min\n",
      "Epoch: 052, LR: 0.000500, Train MAE: 8.8870507, Train RMSE: 10.8083153, Valid MAE: 9.4224581, Valid RMSE: 11.3463421, Best Valid RMSE: 11.2463284, Best Epoch: 051, Time:0.559 min\n",
      "Epoch: 053, LR: 0.000500, Train MAE: 8.8985899, Train RMSE: 10.9067240, Valid MAE: 9.3797896, Valid RMSE: 11.2743845, Best Valid RMSE: 11.2463284, Best Epoch: 051, Time:0.526 min\n",
      "Epoch: 054, LR: 0.000500, Train MAE: 8.9001771, Train RMSE: 10.8126650, Valid MAE: 9.4346186, Valid RMSE: 11.3826685, Best Valid RMSE: 11.2463284, Best Epoch: 051, Time:0.532 min\n",
      "Epoch: 055, LR: 0.000500, Train MAE: 8.8802870, Train RMSE: 10.8416014, Valid MAE: 9.3798477, Valid RMSE: 11.2882996, Best Valid RMSE: 11.2463284, Best Epoch: 051, Time:0.537 min\n",
      "Epoch: 056, LR: 0.000500, Train MAE: 8.8757098, Train RMSE: 10.8065472, Valid MAE: 9.4081687, Valid RMSE: 11.3483992, Best Valid RMSE: 11.2463284, Best Epoch: 051, Time:0.530 min\n",
      "Epoch: 057, LR: 0.000500, Train MAE: 8.8712096, Train RMSE: 10.8318377, Valid MAE: 9.3605007, Valid RMSE: 11.2581835, Best Valid RMSE: 11.2463284, Best Epoch: 051, Time:0.532 min\n",
      "Epoch: 058, LR: 0.000500, Train MAE: 8.8681175, Train RMSE: 10.8197861, Valid MAE: 9.4276804, Valid RMSE: 11.3844614, Best Valid RMSE: 11.2463284, Best Epoch: 051, Time:0.537 min\n",
      "Epoch: 059, LR: 0.000500, Train MAE: 8.8572799, Train RMSE: 10.8215065, Valid MAE: 9.3644572, Valid RMSE: 11.2646456, Best Valid RMSE: 11.2463284, Best Epoch: 051, Time:0.525 min\n",
      "Epoch: 060, LR: 0.000500, Train MAE: 8.8790365, Train RMSE: 10.8127060, Valid MAE: 9.4070070, Valid RMSE: 11.3524122, Best Valid RMSE: 11.2463284, Best Epoch: 051, Time:0.530 min\n",
      "Epoch: 061, LR: 0.000500, Train MAE: 8.8563178, Train RMSE: 10.7997751, Valid MAE: 9.3652189, Valid RMSE: 11.2739840, Best Valid RMSE: 11.2463284, Best Epoch: 051, Time:0.523 min\n",
      "Epoch: 062, LR: 0.000500, Train MAE: 8.8552202, Train RMSE: 10.8104849, Valid MAE: 9.3837475, Valid RMSE: 11.3225641, Best Valid RMSE: 11.2463284, Best Epoch: 051, Time:0.531 min\n",
      "Epoch: 063, LR: 0.000500, Train MAE: 8.8496268, Train RMSE: 10.8130884, Valid MAE: 9.3511583, Valid RMSE: 11.2484455, Best Valid RMSE: 11.2463284, Best Epoch: 051, Time:0.532 min\n",
      "Epoch: 064, LR: 0.000500, Train MAE: 8.8425798, Train RMSE: 10.7881374, Valid MAE: 9.3782422, Valid RMSE: 11.3277159, Best Valid RMSE: 11.2463284, Best Epoch: 051, Time:0.536 min\n",
      "Epoch: 065, LR: 0.000500, Train MAE: 8.8409737, Train RMSE: 10.8107748, Valid MAE: 9.3715342, Valid RMSE: 11.3129444, Best Valid RMSE: 11.2463284, Best Epoch: 051, Time:0.532 min\n",
      "Epoch: 066, LR: 0.000500, Train MAE: 8.8328787, Train RMSE: 10.7747440, Valid MAE: 9.3535045, Valid RMSE: 11.2699337, Best Valid RMSE: 11.2463284, Best Epoch: 051, Time:0.531 min\n",
      "Epoch: 067, LR: 0.000500, Train MAE: 8.8376277, Train RMSE: 10.7607508, Valid MAE: 9.3876967, Valid RMSE: 11.3540573, Best Valid RMSE: 11.2463284, Best Epoch: 051, Time:0.529 min\n",
      "Epoch: 068, LR: 0.000500, Train MAE: 8.8417254, Train RMSE: 10.8488932, Valid MAE: 9.3586223, Valid RMSE: 11.3005648, Best Valid RMSE: 11.2463284, Best Epoch: 051, Time:0.553 min\n",
      "Epoch: 069, LR: 0.000500, Train MAE: 8.8397738, Train RMSE: 10.8151436, Valid MAE: 9.3440963, Valid RMSE: 11.2632399, Best Valid RMSE: 11.2463284, Best Epoch: 051, Time:0.515 min\n",
      "Epoch: 070, LR: 0.000500, Train MAE: 8.8290097, Train RMSE: 10.7593927, Valid MAE: 9.3512875, Valid RMSE: 11.2863512, Best Valid RMSE: 11.2463284, Best Epoch: 051, Time:0.527 min\n",
      "Epoch: 071, LR: 0.000500, Train MAE: 8.8255201, Train RMSE: 10.7744331, Valid MAE: 9.3695660, Valid RMSE: 11.3279152, Best Valid RMSE: 11.2463284, Best Epoch: 051, Time:0.527 min\n",
      "Epoch: 072, LR: 0.000500, Train MAE: 8.8216983, Train RMSE: 10.8115110, Valid MAE: 9.3557435, Valid RMSE: 11.3074417, Best Valid RMSE: 11.2463284, Best Epoch: 051, Time:0.505 min\n",
      "Epoch: 073, LR: 0.000500, Train MAE: 8.8177775, Train RMSE: 10.7597609, Valid MAE: 9.3485724, Valid RMSE: 11.2979889, Best Valid RMSE: 11.2463284, Best Epoch: 051, Time:0.525 min\n",
      "Epoch: 074, LR: 0.000500, Train MAE: 8.8181765, Train RMSE: 10.7972078, Valid MAE: 9.3438077, Valid RMSE: 11.2910013, Best Valid RMSE: 11.2463284, Best Epoch: 051, Time:0.542 min\n",
      "Epoch: 075, LR: 0.000500, Train MAE: 8.8465014, Train RMSE: 10.8169661, Valid MAE: 9.3343388, Valid RMSE: 11.2645864, Best Valid RMSE: 11.2463284, Best Epoch: 051, Time:0.537 min\n",
      "Epoch: 076, LR: 0.000500, Train MAE: 8.8182094, Train RMSE: 10.7963972, Valid MAE: 9.3476102, Valid RMSE: 11.3055191, Best Valid RMSE: 11.2463284, Best Epoch: 051, Time:0.544 min\n",
      "Epoch: 077, LR: 0.000500, Train MAE: 8.8152629, Train RMSE: 10.7920904, Valid MAE: 9.3312077, Valid RMSE: 11.2535219, Best Valid RMSE: 11.2463284, Best Epoch: 051, Time:0.566 min\n",
      "Epoch: 078, LR: 0.000500, Train MAE: 8.7938912, Train RMSE: 10.7427454, Valid MAE: 9.3896066, Valid RMSE: 11.3830233, Best Valid RMSE: 11.2463284, Best Epoch: 051, Time:0.508 min\n",
      "Epoch: 079, LR: 0.000500, Train MAE: 8.8081222, Train RMSE: 10.8321066, Valid MAE: 9.3342285, Valid RMSE: 11.2816420, Best Valid RMSE: 11.2463284, Best Epoch: 051, Time:0.534 min\n",
      "Epoch: 080, LR: 0.000500, Train MAE: 8.7971721, Train RMSE: 10.7518320, Valid MAE: 9.3413587, Valid RMSE: 11.3089933, Best Valid RMSE: 11.2463284, Best Epoch: 051, Time:0.515 min\n",
      "Epoch: 081, LR: 0.000500, Train MAE: 8.7947970, Train RMSE: 10.7710485, Valid MAE: 9.3483830, Valid RMSE: 11.3184996, Best Valid RMSE: 11.2463284, Best Epoch: 051, Time:0.518 min\n",
      "Epoch: 082, LR: 0.000500, Train MAE: 8.7901348, Train RMSE: 10.7992697, Valid MAE: 9.3328450, Valid RMSE: 11.2847776, Best Valid RMSE: 11.2463284, Best Epoch: 051, Time:0.539 min\n",
      "Epoch: 083, LR: 0.000500, Train MAE: 8.7844136, Train RMSE: 10.7548275, Valid MAE: 9.3426507, Valid RMSE: 11.3131199, Best Valid RMSE: 11.2463284, Best Epoch: 051, Time:0.526 min\n",
      "Epoch: 084, LR: 0.000500, Train MAE: 8.7792622, Train RMSE: 10.7564049, Valid MAE: 9.3509262, Valid RMSE: 11.3246641, Best Valid RMSE: 11.2463284, Best Epoch: 051, Time:0.523 min\n",
      "Epoch: 085, LR: 0.000500, Train MAE: 8.7782973, Train RMSE: 10.7549200, Valid MAE: 9.3513984, Valid RMSE: 11.3273954, Best Valid RMSE: 11.2463284, Best Epoch: 051, Time:0.511 min\n",
      "Epoch: 086, LR: 0.000500, Train MAE: 8.7803222, Train RMSE: 10.7678366, Valid MAE: 9.3305468, Valid RMSE: 11.2696667, Best Valid RMSE: 11.2463284, Best Epoch: 051, Time:0.517 min\n",
      "Epoch: 087, LR: 0.000500, Train MAE: 8.7795247, Train RMSE: 10.7401628, Valid MAE: 9.3369785, Valid RMSE: 11.3027906, Best Valid RMSE: 11.2463284, Best Epoch: 051, Time:0.519 min\n",
      "Epoch: 088, LR: 0.000500, Train MAE: 8.7741261, Train RMSE: 10.7521963, Valid MAE: 9.3503031, Valid RMSE: 11.3288231, Best Valid RMSE: 11.2463284, Best Epoch: 051, Time:0.527 min\n",
      "Epoch: 089, LR: 0.000500, Train MAE: 8.7710126, Train RMSE: 10.7571135, Valid MAE: 9.3876265, Valid RMSE: 11.4007816, Best Valid RMSE: 11.2463284, Best Epoch: 051, Time:0.551 min\n",
      "Epoch: 090, LR: 0.000500, Train MAE: 8.7727571, Train RMSE: 10.7613049, Valid MAE: 9.3470057, Valid RMSE: 11.3272591, Best Valid RMSE: 11.2463284, Best Epoch: 051, Time:0.532 min\n",
      "Epoch: 091, LR: 0.000500, Train MAE: 8.7604089, Train RMSE: 10.7434464, Valid MAE: 9.3357783, Valid RMSE: 11.2989979, Best Valid RMSE: 11.2463284, Best Epoch: 051, Time:0.530 min\n",
      "Epoch: 092, LR: 0.000500, Train MAE: 8.7641880, Train RMSE: 10.7624140, Valid MAE: 9.3392068, Valid RMSE: 11.3154354, Best Valid RMSE: 11.2463284, Best Epoch: 051, Time:0.537 min\n",
      "Epoch: 093, LR: 0.000500, Train MAE: 8.7739913, Train RMSE: 10.7399931, Valid MAE: 9.3616169, Valid RMSE: 11.3621845, Best Valid RMSE: 11.2463284, Best Epoch: 051, Time:0.543 min\n",
      "Epoch: 094, LR: 0.000500, Train MAE: 8.7592647, Train RMSE: 10.7470913, Valid MAE: 9.3327658, Valid RMSE: 11.3003092, Best Valid RMSE: 11.2463284, Best Epoch: 051, Time:0.510 min\n",
      "Epoch: 095, LR: 0.000500, Train MAE: 8.7508826, Train RMSE: 10.7483864, Valid MAE: 9.3482635, Valid RMSE: 11.3340731, Best Valid RMSE: 11.2463284, Best Epoch: 051, Time:0.509 min\n",
      "Epoch: 096, LR: 0.000500, Train MAE: 8.7574709, Train RMSE: 10.7425518, Valid MAE: 9.3434068, Valid RMSE: 11.3202801, Best Valid RMSE: 11.2463284, Best Epoch: 051, Time:0.547 min\n",
      "Epoch: 097, LR: 0.000500, Train MAE: 8.7521386, Train RMSE: 10.7474174, Valid MAE: 9.3735877, Valid RMSE: 11.3861551, Best Valid RMSE: 11.2463284, Best Epoch: 051, Time:0.526 min\n",
      "Epoch: 098, LR: 0.000500, Train MAE: 8.7759745, Train RMSE: 10.7640114, Valid MAE: 9.3543482, Valid RMSE: 11.3540354, Best Valid RMSE: 11.2463284, Best Epoch: 051, Time:0.524 min\n",
      "Epoch: 099, LR: 0.000500, Train MAE: 8.7586259, Train RMSE: 10.7214317, Valid MAE: 9.3487788, Valid RMSE: 11.3409367, Best Valid RMSE: 11.2463284, Best Epoch: 051, Time:0.527 min\n"
     ]
    }
   ],
   "source": [
    "#train module\n",
    "import time\n",
    "from datetime import datetime\n",
    "def train(model, train_loader, optimizer, device):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    loss_all = 0\n",
    "    epsilon = 1e-8\n",
    "    # t1 = time.time()\n",
    "    criterion = torch.nn.L1Loss(reduction='none')\n",
    "    total_y_pred = torch.tensor([]).to(device)\n",
    "    total_y_true = torch.tensor([]).to(device)\n",
    "    for data in train_loader:\n",
    "        model = model.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(data)\n",
    "\n",
    "        # pred_loss = F.mse_loss(y_pred, data.y, reduction='none')\n",
    "        # loss = torch.sqrt(pred_loss + epsilon).mean()\n",
    "        # print(data.y[:, args.target])\n",
    "        # y = ((data.y[:, args.target]-mean)/std).view(-1, 1)\n",
    "        y = data['label'].view(-1, 1).to(device)\n",
    "        pred_loss = criterion(y_pred, y)\n",
    "        loss = pred_loss.mean()\n",
    "        total_y_pred = torch.cat((total_y_pred, y_pred))\n",
    "        total_y_true = torch.cat((total_y_true, y))\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)\n",
    "\n",
    "        loss_all += loss.item() * y.size(0) #adverse standardize\n",
    "        \n",
    "        # err = y_pred * std + mean - data.y[:, args.target].view(-1, 1) # nx1 adverse standardize\n",
    "\n",
    "        # total_error = err.abs().sum().item() # float\n",
    "\n",
    "        # loss_all += total_error\n",
    "        optimizer.step()\n",
    "    rmse = torch.sqrt(F.mse_loss(total_y_true, total_y_pred))\n",
    "    return loss_all / len(train_loader.dataset), rmse\n",
    "\n",
    "def eval(model, tst_loader, device):\n",
    "    model.train()\n",
    "\n",
    "    loss_all = 0\n",
    "    # t1 = time.time()\n",
    "    criterion = torch.nn.L1Loss(reduction='none')\n",
    "    total_y_pred = torch.tensor([]).to(device)\n",
    "    total_y_true = torch.tensor([]).to(device)\n",
    "    for data in tst_loader:\n",
    "        model = model.to(DEVICE)\n",
    "        y_pred = model(data).detach()\n",
    "        y = data['label'].view(-1, 1).to(device)\n",
    "        total_y_pred = torch.cat((total_y_pred, y_pred))\n",
    "        total_y_true = torch.cat((total_y_true, y))\n",
    "        pred_loss = criterion(y_pred, y)\n",
    "        loss = pred_loss.mean()\n",
    "        loss_all += loss.item() * y.size(0) #adverse standardize\n",
    "\n",
    "    rmse = torch.sqrt(F.mse_loss(total_y_true, total_y_pred))\n",
    "    \n",
    "    return loss_all / len(tst_loader.dataset), rmse\n",
    "\n",
    "n_epochs = 100\n",
    "lr = 5e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "best_val_mae = float('inf')\n",
    "best_val_rmse = float('inf')\n",
    "best_epoch = 0\n",
    "date = datetime.today().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "for epoch in range(n_epochs):\n",
    "    t_s = time.time()\n",
    "    trn_mae, trn_rmse = train(model, trn_loader, optimizer, DEVICE)\n",
    "    val_mae, val_rmse = eval(model, valid_loader, DEVICE)\n",
    "    t_e = time.time()\n",
    "    if best_val_rmse >= val_rmse:\n",
    "        best_val_rmse = val_rmse\n",
    "        best_epoch = epoch\n",
    "    print(f'Epoch: {epoch:03d}, LR: {lr:5f}, Train MAE: {trn_mae:.7f}, Train RMSE: {trn_rmse:.7f}, Valid MAE: {val_mae:.7f}, Valid RMSE: {val_rmse:.7f}, Best Valid RMSE: {best_val_rmse:.7f}, Best Epoch: {best_epoch:03d}, Time:{(t_e-t_s)/60:.3f} min')\n",
    "# 保存模型的参数\n",
    "torch.save(model.state_dict(), f\"model_record/ecloud_reg_paddle_{date}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get test results\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "model = ecloud_regre(256, 1)  # 先定义模型结构\n",
    "model.load_state_dict(torch.load(\"model_record/ecloud_reg_paddle_2024-09-11-18-07-16.pth\"))   # need to complete with a detailed file name\n",
    "model.eval()\n",
    "model.to(DEVICE)  # 移动模型到设备（例如 GPU）\n",
    "y_pred = torch.tensor([]).to(DEVICE)\n",
    "date = datetime.today().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "\n",
    "for data in tst_loader:\n",
    "    output = model(data)\n",
    "    y_pred = torch.cat((y_pred, output))\n",
    "    \n",
    "test_df = pd.read_csv('data/test.csv')\n",
    "test_df['pred'] = y_pred.detach().cpu().numpy()\n",
    "test_df.to_csv(f'paddle_results/result{date}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "regression via geometric information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.data process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collate function\n",
    "import torch\n",
    "from torch_geometric.data import Data, Batch\n",
    "import numpy as np\n",
    "\n",
    "# class DownstreamCollateFn(object):\n",
    "#     def __init__(self, task_type='regr', is_inference=True):\n",
    "#         self.atom_names = [\"atomic_num\", \"formal_charge\", \"degree\", \"chiral_tag\", \"total_numHs\", \"is_aromatic\", \"hybridization\"]\n",
    "#         self.bond_names = [\"bond_dir\", \"bond_type\", \"is_in_ring\"]\n",
    "#         self.bond_float_names = [\"bond_length\"]\n",
    "#         self.bond_angle_float_names = [\"bond_angle\"]\n",
    "#         self.task_type = task_type\n",
    "#         self.is_inference = is_inference\n",
    "\n",
    "#     def _flat_shapes(self, d):\n",
    "#         for name in d:\n",
    "#             d[name] = d[name].reshape([-1])\n",
    "\n",
    "#     def __call__(self, data_list):\n",
    "#         atom_bond_graph_list = []\n",
    "#         bond_angle_graph_list = []\n",
    "#         compound_class_list = []\n",
    "\n",
    "#         for data in data_list:\n",
    "#             compound_class_list.append(data['Label'])\n",
    "#             data = data['Graph']\n",
    "#             # Atom bond graph construction\n",
    "#             node_features = torch.tensor([data[name].reshape(-1, 1) for name in self.atom_names], dtype=torch.float).squeeze(-1)\n",
    "#             edge_index = torch.tensor(data['edges'], dtype=torch.long).t().contiguous()\n",
    "#             edge_features = torch.tensor([data[name].reshape(-1, 1) for name in self.bond_names + self.bond_float_names], dtype=torch.float).squeeze(-1)\n",
    "\n",
    "#             atom_bond_graph = Data(x=node_features, edge_index=edge_index, edge_attr=edge_features)\n",
    "\n",
    "#             # Bond angle graph construction\n",
    "#             bond_angle_edges = torch.tensor(data['BondAngleGraph_edges'], dtype=torch.long).t().contiguous()\n",
    "#             bond_angle_features = torch.tensor([data[name].reshape(-1, 1) for name in self.bond_angle_float_names], dtype=torch.float).squeeze(-1)\n",
    "\n",
    "#             bond_angle_graph = Data(edge_index=bond_angle_edges, edge_attr=bond_angle_features)\n",
    "\n",
    "#             atom_bond_graph_list.append(atom_bond_graph)\n",
    "#             bond_angle_graph_list.append(bond_angle_graph)\n",
    "\n",
    "#         # Batch the graphs\n",
    "#         atom_bond_graph = Batch.from_data_list(atom_bond_graph_list)\n",
    "#         bond_angle_graph = Batch.from_data_list(bond_angle_graph_list)\n",
    "\n",
    "#         return atom_bond_graph, bond_angle_graph, torch.tensor(compound_class_list, dtype=torch.float32)\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data, Batch\n",
    "\n",
    "class DownstreamCollateFn(object):\n",
    "    def __init__(self, task_type='regr', is_inference=True):\n",
    "        atom_names = [\"atomic_num\", \"formal_charge\", \"degree\", \"chiral_tag\", \"total_numHs\", \"is_aromatic\",\n",
    "                      \"hybridization\"]\n",
    "        bond_names = [\"bond_dir\", \"bond_type\", \"is_in_ring\"]\n",
    "        bond_float_names = [\"bond_length\"]\n",
    "        bond_angle_float_names = [\"bond_angle\"]\n",
    "\n",
    "        self.atom_names = atom_names\n",
    "        self.bond_names = bond_names\n",
    "        self.bond_float_names = bond_float_names\n",
    "        self.bond_angle_float_names = bond_angle_float_names\n",
    "        self.task_type = task_type\n",
    "        self.is_inference = is_inference\n",
    "\n",
    "    def _flat_shapes(self, d):\n",
    "        for name in d:\n",
    "            d[name] = d[name].reshape([-1])\n",
    "\n",
    "    def __call__(self, data_list):\n",
    "        atom_bond_graph_list = []\n",
    "        bond_angle_graph_list = []\n",
    "        compound_class_list = []\n",
    "\n",
    "        for data in data_list:\n",
    "            compound_class_list.append(data['Label'])\n",
    "            graph_data = data['Graph']\n",
    "\n",
    "            # Construct atom-bond graph\n",
    "            num_nodes = len(graph_data[self.atom_names[0]])\n",
    "            edge_index = torch.tensor(graph_data['edges'], dtype=torch.long).t().contiguous()  # edges in PyG are transposed\n",
    "            node_features = {name: torch.tensor(graph_data[name].reshape([-1, 1]), dtype=torch.float) for name in self.atom_names}\n",
    "            edge_features = {name: torch.tensor(graph_data[name].reshape([-1, 1]), dtype=torch.float) for name in self.bond_names + self.bond_float_names}\n",
    "\n",
    "            # Create PyG Data object for atom-bond graph\n",
    "            atom_bond_graph = Data(\n",
    "                x=torch.cat([node_features[name] for name in self.atom_names], dim=-1),  # Combine node features\n",
    "                edge_index=edge_index,\n",
    "                edge_attr=torch.cat([edge_features[name] for name in self.bond_names + self.bond_float_names], dim=-1)  # Combine edge features\n",
    "            )\n",
    "\n",
    "            # Construct bond-angle graph\n",
    "            bond_angle_edges = torch.tensor(graph_data['BondAngleGraph_edges'], dtype=torch.long).t().contiguous()\n",
    "            bond_angle_features = {name: torch.tensor(graph_data[name].reshape([-1, 1]), dtype=torch.float) for name in self.bond_angle_float_names}\n",
    "\n",
    "            # Create PyG Data object for bond-angle graph\n",
    "            bond_angle_graph = Data(\n",
    "                x=None,  # No node features for bond-angle graph\n",
    "                edge_index=bond_angle_edges,\n",
    "                edge_attr=torch.cat([bond_angle_features[name] for name in self.bond_angle_float_names], dim=-1)  # Combine edge features\n",
    "            )\n",
    "\n",
    "            atom_bond_graph_list.append(atom_bond_graph)\n",
    "            bond_angle_graph_list.append(bond_angle_graph)\n",
    "\n",
    "        # Use PyG's Batch to combine graphs into a batch\n",
    "        atom_bond_graph_batch = Batch.from_data_list(atom_bond_graph_list)\n",
    "        bond_angle_graph_batch = Batch.from_data_list(bond_angle_graph_list)\n",
    "\n",
    "        # Return batch of atom-bond graphs, bond-angle graphs, and labels\n",
    "        return atom_bond_graph_batch, bond_angle_graph_batch, torch.tensor(compound_class_list, dtype=torch.float32)\n",
    "\n",
    "\n",
    "class ContrastiveLearningCollateFn(object):\n",
    "    def __init__(self, task_type='regr', is_inference=True):\n",
    "        self.atom_names = [\"atomic_num\", \"formal_charge\", \"degree\", \"chiral_tag\", \"total_numHs\", \"is_aromatic\", \"hybridization\"]\n",
    "        self.bond_names = [\"bond_dir\", \"bond_type\", \"is_in_ring\"]\n",
    "        self.bond_float_names = [\"bond_length\"]\n",
    "        self.bond_angle_float_names = [\"bond_angle\"]\n",
    "        self.task_type = task_type\n",
    "        self.is_inference = is_inference\n",
    "\n",
    "    def _flat_shapes(self, d):\n",
    "        for name in d:\n",
    "            d[name] = d[name].reshape([-1])\n",
    "\n",
    "    def data_to_gs(self, data):\n",
    "        node_features = torch.tensor([data[name].reshape(-1, 1) for name in self.atom_names], dtype=torch.float).squeeze(-1)\n",
    "        edge_index = torch.tensor(data['edges'], dtype=torch.long).t().contiguous()\n",
    "        edge_features = torch.tensor([data[name].reshape(-1, 1) for name in self.bond_names + self.bond_float_names], dtype=torch.float).squeeze(-1)\n",
    "\n",
    "        atom_bond_graph = Data(x=node_features, edge_index=edge_index, edge_attr=edge_features)\n",
    "\n",
    "        bond_angle_edges = torch.tensor(data['BondAngleGraph_edges'], dtype=torch.long).t().contiguous()\n",
    "        bond_angle_features = torch.tensor([data[name].reshape(-1, 1) for name in self.bond_angle_float_names], dtype=torch.float).squeeze(-1)\n",
    "\n",
    "        bond_angle_graph = Data(edge_index=bond_angle_edges, edge_attr=bond_angle_features)\n",
    "\n",
    "        return atom_bond_graph, bond_angle_graph\n",
    "\n",
    "    def __call__(self, data_list):\n",
    "        anchor_atom_bond_graph_list = []\n",
    "        anchor_bond_angle_graph_list = []\n",
    "        positive_atom_bond_graph_list = []\n",
    "        positive_bond_angle_graph_list = []\n",
    "        negative_atom_bond_graph_list = []\n",
    "        negative_bond_angle_graph_list = []\n",
    "        compound_class_list = []\n",
    "\n",
    "        for data in data_list:\n",
    "            anchor = data[0]\n",
    "            positive = data[1]\n",
    "            negative = data[2]\n",
    "            compound_class_list.append(anchor['Label'])\n",
    "\n",
    "            anchor_ab_g, anchor_ba_g = self.data_to_gs(anchor['Graph'])\n",
    "            positive_ab_g, positive_ba_g = self.data_to_gs(positive['Graph'])\n",
    "            negative_ab_g, negative_ba_g = self.data_to_gs(negative['Graph'])\n",
    "\n",
    "            anchor_atom_bond_graph_list.append(anchor_ab_g)\n",
    "            anchor_bond_angle_graph_list.append(anchor_ba_g)\n",
    "            positive_atom_bond_graph_list.append(positive_ab_g)\n",
    "            positive_bond_angle_graph_list.append(positive_ba_g)\n",
    "            negative_atom_bond_graph_list.append(negative_ab_g)\n",
    "            negative_bond_angle_graph_list.append(negative_ba_g)\n",
    "\n",
    "        # Batch the graphs\n",
    "        anchor_atom_bond_graph = Batch.from_data_list(anchor_atom_bond_graph_list)\n",
    "        anchor_bond_angle_graph = Batch.from_data_list(anchor_bond_angle_graph_list)\n",
    "        positive_atom_bond_graph = Batch.from_data_list(positive_atom_bond_graph_list)\n",
    "        positive_bond_angle_graph = Batch.from_data_list(positive_bond_angle_graph_list)\n",
    "        negative_atom_bond_graph = Batch.from_data_list(negative_atom_bond_graph_list)\n",
    "        negative_bond_angle_graph = Batch.from_data_list(negative_bond_angle_graph_list)\n",
    "\n",
    "        return (anchor_atom_bond_graph, anchor_bond_angle_graph,\n",
    "                positive_atom_bond_graph, positive_bond_angle_graph,\n",
    "                negative_atom_bond_graph, negative_bond_angle_graph,\n",
    "                torch.tensor(compound_class_list, dtype=torch.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset\n",
    "import os\n",
    "from os.path import join, exists\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# from pgl.utils.data import Dataloader\n",
    "from torch_geometric.loader import DataLoader  # pyg 的数据加载器\n",
    "# from pahelix.utils.data_utils import save_data_list_to_npz, load_npz_to_data_list\n",
    "# from pahelix.utils.basic_utils import mp_pool_map\n",
    "from torch.multiprocessing import Pool  # 用于并行化操作\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from pyscf import gto, scf, tools\n",
    "import numpy as np\n",
    "from scipy.ndimage import zoom\n",
    "import numpy as np\n",
    "import torch\n",
    "import h5py\n",
    "from coati.models.encoding.tokenizers.trie_tokenizer import TrieTokenizer\n",
    "from coati.models.encoding.tokenizers import get_vocab\n",
    "__all__ = ['PaddleDataset3d']\n",
    "\n",
    "\n",
    "class PaddleDataset3d(object):\n",
    "    def __init__(self, \n",
    "            data_list=None,\n",
    "            npz_data_path=None,\n",
    "            npz_data_files=None):\n",
    "\n",
    "        super(PaddleDataset3d, self).__init__()\n",
    "        self.data_list = data_list\n",
    "        self.npz_data_path = npz_data_path\n",
    "        self.npz_data_files = npz_data_files\n",
    "\n",
    "        if not npz_data_path is None:\n",
    "            self.data_list = self._load_npz_data_path(npz_data_path)\n",
    "\n",
    "        if not npz_data_files is None:\n",
    "            self.data_list = self._load_npz_data_files(npz_data_files)\n",
    "\n",
    "    def _load_npz_data_path(self, data_path):\n",
    "        data_list = []\n",
    "        files = [f for f in os.listdir(data_path) if f.endswith('.npz')]\n",
    "        files = sorted(files)\n",
    "        for f in files:\n",
    "            # data_list += load_npz_to_data_list(join(data_path, f))\n",
    "            data_list += np.ndarray.tolist(join(data_path, f))\n",
    "        return data_list\n",
    "\n",
    "    def _load_npz_data_files(self, data_files):\n",
    "        data_list = []\n",
    "        for f in data_files:\n",
    "            # data_list += load_npz_to_data_list(f)\n",
    "            data_list += np.ndarray.tolist(f)\n",
    "        return data_list\n",
    "\n",
    "    def _save_npz_data(self, data_list, data_path, max_num_per_file=10000):\n",
    "        if not exists(data_path):\n",
    "            os.makedirs(data_path)\n",
    "        n = len(data_list)\n",
    "        for i in range(int((n - 1) / max_num_per_file) + 1):\n",
    "            filename = 'part-%06d.npz' % i\n",
    "            sub_data_list = self.data_list[i * max_num_per_file: (i + 1) * max_num_per_file]\n",
    "            np.savez(sub_data_list, join(data_path, filename))\n",
    "\n",
    "    def save_data(self, data_path):\n",
    "        \"\"\"\n",
    "        Save the ``data_list`` to the disk specified by ``data_path`` with npz format.\n",
    "        After that, call `InMemoryDataset(data_path)` to reload the ``data_list``.\n",
    "\n",
    "        Args:\n",
    "            data_path(str): the path to the cached npz path.\n",
    "        \"\"\"\n",
    "        self._save_npz_data(self.data_list, data_path)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        if isinstance(key, slice):\n",
    "            start, stop, step = key.indices(len(self))\n",
    "            dataset = PaddleDataset3d(\n",
    "                    data_list=[self[i] for i in range(start, stop, step)])\n",
    "            return dataset\n",
    "        elif isinstance(key, int) or \\\n",
    "                isinstance(key, np.int64) or \\\n",
    "                isinstance(key, np.int32):\n",
    "            return self.data_list[key]\n",
    "        elif isinstance(key, list):\n",
    "            dataset = PaddleDataset3d(\n",
    "                    data_list=[self[i] for i in key])\n",
    "            return dataset\n",
    "        else:\n",
    "            raise TypeError('Invalid argument type: %s of %s' % (type(key), key))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def transform(self, transform_fn, num_workers=1, drop_none=False):\n",
    "        \"\"\"\n",
    "        Inplace apply `transform_fn` on the `data_list` with multiprocess.\n",
    "        \"\"\"\n",
    "        with Pool(num_workers) as pool:\n",
    "            data_list = pool.map(transform_fn, self.data_list)\n",
    "        if drop_none:\n",
    "            self.data_list = [data for data in data_list if data is not None]\n",
    "        else:\n",
    "            self.data_list = data_list\n",
    "            \n",
    "    def read_cube_file(self, file_path):\n",
    "        with open(file_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        atom_line = lines[2].split()     # 跳过前两行（注释行）\n",
    "        num_atoms = int(atom_line[0])    # 获取原子数和原点坐标\n",
    "\n",
    "        grid_info = [list(map(float, lines[i].split())) for i in range(3, 6)]     # 第三行到第五行分别是网格信息\n",
    "        grid_shape = [int(abs(info[0])) for info in grid_info]  # 获取网格维度\n",
    "        \n",
    "        origin = np.array(grid_info)[:, 1:4]     # 网格大小和原点\n",
    "        \n",
    "        # atom_info = [list(map(float, lines[i].split())) for i in range(6, 6 + num_atoms)]     # 原子信息 (读取接下来 num_atoms 行的数据)\n",
    "        atom_info = []\n",
    "        mol_coords = []\n",
    "        mol_atomic = []\n",
    "        for i in range(6, 6 + num_atoms):\n",
    "            line_data = list(map(float, lines[i].split()))\n",
    "            atomic_number = int(line_data[0])  # 原子序号\n",
    "            coordinates = line_data[2:5]  # x, y, z 坐标\n",
    "            mol_coords.append(coordinates)\n",
    "            mol_atomic.append(atomic_number)\n",
    "        atom_info.append({\n",
    "            'atomic_number': mol_atomic,\n",
    "            'coordinates': mol_coords\n",
    "        })\n",
    "        density_data = []\n",
    "        for line in lines[6 + num_atoms:]:\n",
    "            density_data.extend(map(float, line.split()))\n",
    "        \n",
    "        density_data = np.array(density_data).reshape(grid_shape)\n",
    "        \n",
    "        return density_data, atom_info, origin, grid_shape\n",
    "\n",
    "    def sml2ecloud(self, smiles):\n",
    "        '''\n",
    "        smiles: list of smile stringa\n",
    "        return: h5 file, \n",
    "        '''\n",
    "        eclouds = torch.tensor([])\n",
    "        atomic_number = torch.tensor([])\n",
    "        coords = torch.tensor([])\n",
    "        augmented_tokens = torch.tensor([])\n",
    "        tokenizer = TrieTokenizer(n_seq=128, **get_vocab('mar'))\n",
    "        for smile in smiles:\n",
    "            mol = Chem.MolFromSmiles(smile)\n",
    "\n",
    "            # mol to points\n",
    "            mol = Chem.AddHs(mol)  # 添加氢原子\n",
    "            AllChem.EmbedMolecule(mol)  # 生成3D坐标\n",
    "            AllChem.UFFOptimizeMolecule(mol)  # 用UFF力场优化\n",
    "            \n",
    "            conf = mol.GetConformer()\n",
    "            xyz = \"\"\n",
    "            for i, atom in enumerate(mol.GetAtoms()):\n",
    "                pos = conf.GetAtomPosition(i)\n",
    "                xyz += f\"{atom.GetSymbol()} {pos.x} {pos.y} {pos.z}\\n\"\n",
    "\n",
    "            mol = gto.M(atom=xyz, basis=\"sto-3g\")  # 定义分子并选择基组\n",
    "            mf = scf.RHF(mol)  # Hartree-Fock计算\n",
    "            mf.kernel()  # 运行计算\n",
    "\n",
    "            # 生成电子密度\n",
    "            tools.cubegen.density(mol, f'examples/eclouds_{smile}.cube', mf.make_rdm1())\n",
    "            # # 添加 batch 维度和 channel 维度，形状为 (1, 1, x, y, z)\n",
    "            # density_tensor = density_tensor.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "            # print(density_tensor.shape)  # 输出形状为 (1, 1, x, y, z)\n",
    "\n",
    "            # print(\"电子密度网格形状：\", density_data.shape)\n",
    "            # print(\"原子信息：\", atom_info)\n",
    "            # print(\"网格原点：\", origin)\n",
    "\n",
    "            ecloud_density, atom_info, origin, grid_shape = self.read_cube_file(f'paddle_data/ecloud/eclouds_{smile}.cube')\n",
    "            print('atom info', atom_info)\n",
    "            n = ecloud_density.shape[0]  # get size of raw ecloud\n",
    "            target_shape = (32, 32, 32) # 使用 scipy 的 zoom 函数将电子密度插值到 (32, 32, 32)\n",
    "            ecloud = zoom(ecloud_density, (target_shape[0] / n, target_shape[1] / n, target_shape[2] / n))\n",
    "            \n",
    "            ecloud = torch.tensor(ecloud, dtype=torch.double).unsqueeze(0)\n",
    "            mol_atomic = torch.tensor(atom_info[0]['atomic_number']).unsqueeze(0)\n",
    "            mol_coords = torch.tensor(atom_info[0]['coordinates']).unsqueeze(0)\n",
    "            augmented_token = torch.tensor(tokenizer.tokenize_text(\"[CLIP][UNK][SMILES][SUFFIX][MIDDLE]\" + smile + \"[STOP]\", pad=True)).unsqueeze(0)\n",
    "            \n",
    "            eclouds = torch.cat((eclouds, ecloud), dim=0)\n",
    "            atomic_number = torch.cat((atomic_number, mol_atomic), dim=0)\n",
    "            coords = torch.cat((coords, mol_coords), dim=0)\n",
    "            augmented_tokens = torch.cat((augmented_tokens, augmented_token))\n",
    "        # 假设 density_32 是 (32, 32, 32) 的 3D 电子云数据\n",
    "        with h5py.File(f'paddle_data/ecloud/eclouds_{len(smiles)}.h5', 'w') as f:\n",
    "            f.create_dataset('ecloud', data=eclouds)\n",
    "            f.create_dataset('atomic_number', data=atomic_number)\n",
    "            f.create_dataset('coords', data=coords)\n",
    "            f.create_dataset('smiles', data=smiles)\n",
    "            f.create_dataset('augmented_tokens', data=augmented_tokens)\n",
    "\n",
    "\n",
    "    def get_data_loader(self, batch_size, num_workers=1, shuffle=False, collate_fn=None):\n",
    "        data_list = []\n",
    "        i = 0\n",
    "        for data in self.data_list:\n",
    "            atomic_number = torch.tensor(data['Graph']['atomic_num'], dtype=torch.long)\n",
    "            pos = torch.tensor(data['Graph']['atom_pos'], dtype=torch.float)\n",
    "                \n",
    "            edge_index = torch.tensor(data['Graph']['edges'], dtype=torch.long).view(2, -1)\n",
    "            edge_type = torch.tensor(data['Graph']['bond_type'], dtype=torch.long).view(-1, 1)\n",
    "            edge_length = torch.tensor(data['Graph']['bond_length'], dtype=torch.float).view(-1, 1)\n",
    "            \n",
    "            edge_angle = torch.tensor(data['Graph']['bond_angle'], dtype=torch.float).view(-1, 1)\n",
    "            edge_angle_index = torch.tensor(data['Graph']['BondAngleGraph_edges'], dtype=torch.long).view(2, -1)\n",
    "            \n",
    "            label = torch.tensor(data['Label'], dtype=torch.float).view(-1, 1)\n",
    "\n",
    "            name = data['Smiles']\n",
    "            \n",
    "            per_data = Data(atomic_number=atomic_number, pos=pos, edge_index=edge_index, \n",
    "                edge_type=edge_type, edge_length=edge_length,y=label, name=name, index=i, \n",
    "                edge_angle_index=edge_angle_index, edge_angle=edge_angle)\n",
    "            data_list.append(per_data)\n",
    "            i += 1\n",
    "        return DataLoader(data_list, \n",
    "                batch_size=batch_size, \n",
    "                num_workers=num_workers, \n",
    "                shuffle=shuffle,\n",
    "                collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataloader\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def get_data_loader(mode, batch_size=256):\n",
    "    collate_fn = DownstreamCollateFn()\n",
    "    if mode == 'train':\n",
    "        data_list = pickle.load(open(\"data/conformation/train.pkl\", 'rb'))\n",
    "\n",
    "        train, valid = train_test_split(data_list, random_state=42, test_size=0.2)\n",
    "        # print('train', train[0], '\\n valid', valid[0])\n",
    "        train, valid = PaddleDataset3d(train), PaddleDataset3d(valid)\n",
    "        # print('train', train[0], '\\n valid', valid[0])\n",
    "\n",
    "        print(f'len train is {len(train)}, len valid is {len(valid)}')\n",
    "\n",
    "        train_dl = train.get_data_loader(batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "        valid_dl = valid.get_data_loader(batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "        return train_dl, valid_dl\n",
    "    elif mode == 'test':\n",
    "        data_list = pickle.load(open(\"data/conformation/test.pkl\", 'rb'))\n",
    "\n",
    "        print(f'len test is {len(data_list)}')\n",
    "\n",
    "        test = PaddleDataset3d(data_list)\n",
    "        test_dl = test.get_data_loader(batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "        return test_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len train is 1381, len valid is 346\n"
     ]
    }
   ],
   "source": [
    "train_data_loader, valid_data_loader = get_data_loader(mode='train', batch_size=2)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C[C@H]1[C+]2CC[C@@H]1[C@@]1(C)CC[C@@H](C2)C1(C)C', 'C[C@@H]1CC[C@H]2[C@@H](C1)[C@]1(C)CCC[CH+][C@H]2C1']\n"
     ]
    }
   ],
   "source": [
    "for data in train_data_loader:\n",
    "    print(data.name)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.points-ecloud encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecloudgen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
