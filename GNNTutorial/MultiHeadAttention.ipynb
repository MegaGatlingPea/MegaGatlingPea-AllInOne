{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Head Attention Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a matrix in T*d, where T is the number of nodes, d is the dimension of each node's feature\n",
    "X = torch.randn(10, 16) # T=10 hidden_size=16\n",
    "W_Q = nn.Linear(16, 16) # hidden_size = 16\n",
    "W_K = nn.Linear(16, 16) # hidden_size = 16\n",
    "W_V = nn.Linear(16, 16) # hidden_size = 16\n",
    "\n",
    "# Query, Key, Value\n",
    "Q = W_Q(X) # Q: [10, 16]\n",
    "K = W_K(X) # K: [10, 16]\n",
    "V = W_V(X) # V: [10, 16]\n",
    "\n",
    "# Perform Attention Mechanism\n",
    "# dot product\n",
    "dot = torch.matmul(Q, K.transpose(-2, -1)) # QÂ·K^T: [10, 10]\n",
    "\n",
    "# Scale the dot product\n",
    "dot = dot / torch.sqrt(torch.tensor(16, dtype=torch.float32))\n",
    "\n",
    "# Masked Attention (Opt.)\n",
    "# should be -np.inf before softmax, adj matrix can be used here\n",
    "mask = torch.zeros(10, 10)\n",
    "mask[0, 2] = -torch.inf; mask[2, 0] = -torch.inf\n",
    "\n",
    "# Softmax\n",
    "attention = torch.softmax(dot + mask, dim=1) # [10, 10]\n",
    "\n",
    "# It should pass the assert since we masked the attention between node 0 and 2\n",
    "assert attention[0, 2] == 0 and attention[2, 0] == 0\n",
    "\n",
    "# MatMul\n",
    "Z = torch.matmul(attention, V) # [10, 16]\n",
    "\n",
    "# Output\n",
    "W_O = nn.Linear(16, 16) # hidden_size = 16\n",
    "output = W_O(Z) # [10, 16]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Head Attention Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_size=16, attention_dropout_rate=0):\n",
    "        \n",
    "        super(SingleHeadAttention, self).__init__()\n",
    "        self.linear_q = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear_k = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear_v = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear_o = nn.Linear(hidden_size, hidden_size)\n",
    "        self.scale = hidden_size ** -0.5\n",
    "        self.att_dropout = nn.Dropout(attention_dropout_rate)\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        q = self.linear_q(x)\n",
    "        k = self.linear_k(x)\n",
    "        v = self.linear_v(x)\n",
    "        x = torch.matmul(q, k.transpose(-2, -1))\n",
    "        x = x * self.scale\n",
    "        if attn_mask is not None:\n",
    "            x = x + attn_mask\n",
    "        x = torch.softmax(x, dim=1)\n",
    "        x = self.att_dropout(x)\n",
    "        x = torch.matmul(x, v)\n",
    "        x = self.linear_o(x)\n",
    "        return x\n",
    "\n",
    "# create a single head attention instance the same as the above without mask\n",
    "single_head_attention = SingleHeadAttention()\n",
    "\n",
    "# create a random tensor with shape (10, 16)\n",
    "x = torch.randn(10, 16)\n",
    "\n",
    "# forward pass\n",
    "output = single_head_attention(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Head Attention Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Input and Weights\n",
    "X = torch.randn(10, 16) \n",
    "\n",
    "W_Q = nn.Linear(16, 16)\n",
    "W_K = nn.Linear(16, 16)\n",
    "W_V = nn.Linear(16, 16)\n",
    "\n",
    "# Multi Head Attention\n",
    "num_heads = 4\n",
    "att_size = 16 // num_heads\n",
    "\n",
    "Q = W_Q(X).view(10, num_heads, att_size) # [10, 4, 4]\n",
    "K = W_K(X).view(10, num_heads, att_size) # [10, 4, 4]\n",
    "V = W_V(X).view(10, num_heads, att_size) # [10, 4, 4]\n",
    "\n",
    "# permute the Q, K, V to [num_heads, num_samples, num_features] to satisfy torch.matmul's requirement\n",
    "Q = Q.permute(1, 0, 2)\n",
    "K = K.permute(1, 0, 2)\n",
    "V = V.permute(1, 0, 2)\n",
    "\n",
    "# dot product\n",
    "dot = torch.matmul(Q, K.transpose(-2, -1))\n",
    "\n",
    "# Scale the dot product\n",
    "dot = dot * att_size ** -0.5\n",
    "\n",
    "# Softmax\n",
    "attention = torch.softmax(dot, dim=2)\n",
    "\n",
    "# Calculate Z\n",
    "Z = torch.matmul(attention, V)\n",
    "Z = Z.permute(1,0,2) # [num_samples, num_heads, num_features]\n",
    "Z_concat = Z.contiguous().view(10, -1)  # [10, 16]\n",
    "\n",
    "# Output\n",
    "W_O = nn.Linear(16, 16) # hidden_size = 16\n",
    "output = W_O(Z_concat) # [10, 16]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Head Attention Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_size=16, num_heads=4, attention_dropout_rate=0):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.att_size = hidden_size // num_heads\n",
    "        self.linear_q = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear_k = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear_v = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear_o = nn.Linear(hidden_size, hidden_size)\n",
    "        self.att_dropout = nn.Dropout(attention_dropout_rate)\n",
    "        \n",
    "    def forward(self, x, attn_mask=None):\n",
    "        q = self.linear_q(x).view(x.size(0), self.num_heads, self.att_size)\n",
    "        k = self.linear_k(x).view(x.size(0), self.num_heads, self.att_size)\n",
    "        v = self.linear_v(x).view(x.size(0), self.num_heads, self.att_size)\n",
    "        \n",
    "        q = q.permute(1, 0, 2)\n",
    "        k = k.permute(1, 0, 2)\n",
    "        v = v.permute(1, 0, 2)\n",
    "        \n",
    "        dot = torch.matmul(q, k.transpose(-2, -1))\n",
    "        dot = dot * self.att_size ** -0.5\n",
    "        \n",
    "        if attn_mask is not None:\n",
    "            dot = dot + attn_mask\n",
    "        \n",
    "        attention = torch.softmax(dot, dim=2)\n",
    "        attention = self.att_dropout(attention)\n",
    "        \n",
    "        Z = torch.matmul(attention, v)\n",
    "        Z = Z.permute(1, 0, 2)\n",
    "        Z = Z.contiguous().view(x.size(0), -1)\n",
    "        output = self.linear_o(Z)\n",
    "        return output\n",
    "\n",
    "# create a multi head attention instance the same as the above without mask\n",
    "multi_head_attention = MultiHeadAttention()\n",
    "\n",
    "# create a random tensor with shape (10, 16)\n",
    "x = torch.randn(10, 16)\n",
    "\n",
    "# forward pass\n",
    "output = multi_head_attention(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zerobind",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
