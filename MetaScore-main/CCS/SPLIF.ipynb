{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of complexes selected: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing complexes: 100%|██████████| 50/50 [00:37<00:00,  1.33it/s]\n",
      "Calculating similarities: 100%|██████████| 50/50 [00:00<00:00, 1561.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity matrix shape: (50, 50)\n",
      "Sample of similarity matrix:\n",
      "[[1.         0.01486989 0.00342466 0.02312139 0.01038961]\n",
      " [0.01486989 1.         0.0130719  0.03055556 0.01754386]\n",
      " [0.00342466 0.0130719  1.         0.0801105  0.04668305]\n",
      " [0.02312139 0.03055556 0.0801105  1.         0.0993228 ]\n",
      " [0.01038961 0.01754386 0.04668305 0.0993228  1.        ]]\n",
      "Results saved to 'splif_similarity_matrix_50.npy' and 'pdb_ids_50.txt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import oddt\n",
    "import numpy as np\n",
    "from oddt.toolkits import ob\n",
    "from oddt.fingerprints import SPLIF\n",
    "from typing import Tuple, List, Dict\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "def splif_to_bitvec_and_weights(splif_result: np.ndarray, size: int = 4096) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    bitvec = np.zeros(size, dtype=bool)\n",
    "    weights = np.zeros(size, dtype=int)\n",
    "    \n",
    "    unique_hashes, counts = np.unique(splif_result['hash'], return_counts=True)\n",
    "    bitvec[unique_hashes] = True\n",
    "    weights[unique_hashes] = counts\n",
    "    \n",
    "    return bitvec, weights\n",
    "\n",
    "def weighted_tanimoto_similarity(splif1: np.ndarray, splif2: np.ndarray, size: int = 4096) -> float:\n",
    "    bitvec1, weights1 = splif_to_bitvec_and_weights(splif1, size)\n",
    "    bitvec2, weights2 = splif_to_bitvec_and_weights(splif2, size)\n",
    "    \n",
    "    common_bits = bitvec1 & bitvec2\n",
    "    weighted_intersection = np.sum(np.minimum(weights1[common_bits], weights2[common_bits]))\n",
    "    weighted_union = np.sum(np.maximum(weights1, weights2))\n",
    "    \n",
    "    return weighted_intersection / weighted_union if weighted_union > 0 else 0.0\n",
    "\n",
    "def load_molecule(file_path: str, file_type: str) -> oddt.toolkit.Molecule:\n",
    "    molecule = next(ob.readfile(file_type, file_path))\n",
    "    if file_type == 'pdb':\n",
    "        molecule.protein = True\n",
    "    return molecule\n",
    "\n",
    "def calculate_splif(ligand: oddt.toolkit.Molecule, protein: oddt.toolkit.Molecule) -> np.ndarray:\n",
    "    return SPLIF(ligand, protein)\n",
    "\n",
    "def process_complex(pdb_path: str, sdf_path: str) -> np.ndarray:\n",
    "    protein = load_molecule(pdb_path, 'pdb')\n",
    "    ligand = load_molecule(sdf_path, 'sdf')\n",
    "    return calculate_splif(ligand, protein)\n",
    "\n",
    "def get_pdb_ids(base_path: str, n: int = None) -> List[str]:\n",
    "    all_pdb_ids = [d for d in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, d))]\n",
    "    if n is None or n >= len(all_pdb_ids):\n",
    "        return all_pdb_ids\n",
    "    return random.sample(all_pdb_ids, n)\n",
    "\n",
    "def calculate_similarity_matrix(base_path: str, pdb_ids: List[str]) -> np.ndarray:\n",
    "    n = len(pdb_ids)\n",
    "    similarity_matrix = np.zeros((n, n))\n",
    "    splif_fingerprints = {}\n",
    "\n",
    "    for i, pdb_id in enumerate(tqdm(pdb_ids, desc=\"Processing complexes\")):\n",
    "        pdb_path = os.path.join(base_path, pdb_id, f\"{pdb_id}_protein.pdb\")\n",
    "        sdf_path = os.path.join(base_path, pdb_id, f\"{pdb_id}_ligand.sdf\")\n",
    "        splif_fingerprints[pdb_id] = process_complex(pdb_path, sdf_path)\n",
    "\n",
    "    for i in tqdm(range(n), desc=\"Calculating similarities\"):\n",
    "        for j in range(i, n):\n",
    "            similarity = weighted_tanimoto_similarity(\n",
    "                splif_fingerprints[pdb_ids[i]], \n",
    "                splif_fingerprints[pdb_ids[j]]\n",
    "            )\n",
    "            similarity_matrix[i, j] = similarity_matrix[j, i] = similarity\n",
    "\n",
    "    return similarity_matrix\n",
    "\n",
    "def main():\n",
    "    base_path = '/mnt/data/pdbbind2020-PL'\n",
    "    n_complexes = 50  \n",
    "    \n",
    "    pdb_ids = get_pdb_ids(base_path, n_complexes)\n",
    "    \n",
    "    print(f\"Number of complexes selected: {len(pdb_ids)}\")\n",
    "    \n",
    "    similarity_matrix = calculate_similarity_matrix(base_path, pdb_ids)\n",
    "    \n",
    "    print(\"Similarity matrix shape:\", similarity_matrix.shape)\n",
    "    print(\"Sample of similarity matrix:\")\n",
    "    print(similarity_matrix[:5, :5])\n",
    "\n",
    "    # results\n",
    "    np.save(f\"splif_similarity_matrix_{len(pdb_ids)}.npy\", similarity_matrix)\n",
    "    with open(f\"pdb_ids_{len(pdb_ids)}.txt\", \"w\") as f:\n",
    "        for pdb_id in pdb_ids:\n",
    "            f.write(f\"{pdb_id}\\n\")\n",
    "\n",
    "    print(f\"Results saved to 'splif_similarity_matrix_{len(pdb_ids)}.npy' and 'pdb_ids_{len(pdb_ids)}.txt'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cheminfo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
