{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "{'support_set': [(Data(x=[545, 1280], edge_index=[2, 5912], pos=[545, 3]), Data(x=[22, 9], edge_index=[2, 44], edge_attr=[44, 3], pos=[22, 3]), tensor([8.3400]), '3lj7'), (Data(x=[317, 1280], edge_index=[2, 3094], pos=[317, 3]), Data(x=[31, 9], edge_index=[2, 62], edge_attr=[62, 3], pos=[31, 3]), tensor([1.7000]), '6cex'), (Data(x=[426, 1280], edge_index=[2, 4122], pos=[426, 3]), Data(x=[98, 9], edge_index=[2, 204], edge_attr=[204, 3], pos=[98, 3]), tensor([8.0600]), '5ea5'), (Data(x=[387, 1280], edge_index=[2, 4050], pos=[387, 3]), Data(x=[72, 9], edge_index=[2, 150], edge_attr=[150, 3], pos=[72, 3]), tensor([6.3700]), '4r91'), (Data(x=[330, 1280], edge_index=[2, 3000], pos=[330, 3]), Data(x=[39, 9], edge_index=[2, 78], edge_attr=[78, 3], pos=[39, 3]), tensor([4.8200]), '4e6c'), (Data(x=[223, 1280], edge_index=[2, 2350], pos=[223, 3]), Data(x=[24, 9], edge_index=[2, 48], edge_attr=[48, 3], pos=[24, 3]), tensor([2.8500]), '3rxa'), (Data(x=[215, 1280], edge_index=[2, 2220], pos=[215, 3]), Data(x=[59, 9], edge_index=[2, 124], edge_attr=[124, 3], pos=[59, 3]), tensor([8.2200]), '1u9v'), (Data(x=[223, 1280], edge_index=[2, 2356], pos=[223, 3]), Data(x=[24, 9], edge_index=[2, 48], edge_attr=[48, 3], pos=[24, 3]), tensor([2.8500]), '3atk'), (Data(x=[324, 1280], edge_index=[2, 3122], pos=[324, 3]), Data(x=[31, 9], edge_index=[2, 62], edge_attr=[62, 3], pos=[31, 3]), tensor([1.7000]), '6cf5'), (Data(x=[835, 1280], edge_index=[2, 7786], pos=[835, 3]), Data(x=[62, 9], edge_index=[2, 130], edge_attr=[130, 3], pos=[62, 3]), tensor([8.1800]), '3oe9')], 'query_set': [(Data(x=[213, 1280], edge_index=[2, 2260], pos=[213, 3]), Data(x=[57, 9], edge_index=[2, 118], edge_attr=[118, 3], pos=[57, 3]), tensor([7.]), '3kx1'), (Data(x=[658, 1280], edge_index=[2, 7094], pos=[658, 3]), Data(x=[31, 9], edge_index=[2, 62], edge_attr=[62, 3], pos=[31, 3]), tensor([3.3000]), '2vw2'), (Data(x=[249, 1280], edge_index=[2, 2738], pos=[249, 3]), Data(x=[38, 9], edge_index=[2, 76], edge_attr=[76, 3], pos=[38, 3]), tensor([5.1500]), '1qxw'), (Data(x=[386, 1280], edge_index=[2, 3600], pos=[386, 3]), Data(x=[36, 9], edge_index=[2, 74], edge_attr=[74, 3], pos=[36, 3]), tensor([4.8000]), '5j3l'), (Data(x=[410, 1280], edge_index=[2, 4000], pos=[410, 3]), Data(x=[47, 9], edge_index=[2, 102], edge_attr=[102, 3], pos=[47, 3]), tensor([9.7700]), '5lxb'), (Data(x=[251, 1280], edge_index=[2, 2626], pos=[251, 3]), Data(x=[27, 9], edge_index=[2, 54], edge_attr=[54, 3], pos=[27, 3]), tensor([2.7000]), '6nmb'), (Data(x=[318, 1280], edge_index=[2, 3068], pos=[318, 3]), Data(x=[40, 9], edge_index=[2, 84], edge_attr=[84, 3], pos=[40, 3]), tensor([7.3500]), '4pm0'), (Data(x=[389, 1280], edge_index=[2, 4066], pos=[389, 3]), Data(x=[25, 9], edge_index=[2, 52], edge_attr=[52, 3], pos=[25, 3]), tensor([2.4000]), '4zsm'), (Data(x=[836, 1280], edge_index=[2, 7652], pos=[836, 3]), Data(x=[62, 9], edge_index=[2, 130], edge_attr=[130, 3], pos=[62, 3]), tensor([8.0900]), '3oe6'), (Data(x=[248, 1280], edge_index=[2, 2256], pos=[248, 3]), Data(x=[68, 9], edge_index=[2, 142], edge_attr=[142, 3], pos=[68, 3]), tensor([4.2300]), '3zqe'), (Data(x=[290, 1280], edge_index=[2, 2792], pos=[290, 3]), Data(x=[34, 9], edge_index=[2, 72], edge_attr=[72, 3], pos=[34, 3]), tensor([4.9200]), '1e1v'), (Data(x=[922, 1280], edge_index=[2, 9526], pos=[922, 3]), Data(x=[31, 9], edge_index=[2, 62], edge_attr=[62, 3], pos=[31, 3]), tensor([1.6200]), '5k5c'), (Data(x=[223, 1280], edge_index=[2, 2358], pos=[223, 3]), Data(x=[22, 9], edge_index=[2, 44], edge_attr=[44, 3], pos=[22, 3]), tensor([2.1500]), '3rxg'), (Data(x=[346, 1280], edge_index=[2, 3416], pos=[346, 3]), Data(x=[34, 9], edge_index=[2, 68], edge_attr=[68, 3], pos=[34, 3]), tensor([2.9200]), '5ct2'), (Data(x=[1020, 1280], edge_index=[2, 10658], pos=[1020, 3]), Data(x=[48, 9], edge_index=[2, 98], edge_attr=[98, 3], pos=[48, 3]), tensor([8.0600]), '4avt')]}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import lmdb\n",
    "import pickle\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MetaDataset(Dataset):\n",
    "    def __init__(self, cluster_data_dir, k_shot, k_query):\n",
    "        \"\"\"\n",
    "        Initialize MetaDataset, for meta-learning.\n",
    "        \n",
    "        Args:\n",
    "            cluster_data_dir (str): directory path containing all cluster LMDB files.\n",
    "            k_shot (int): number of samples in support set.\n",
    "            k_query (int): number of samples in query set.\n",
    "        \"\"\"\n",
    "        self.cluster_data_dir = cluster_data_dir\n",
    "        self.k_shot = k_shot\n",
    "        self.k_query = k_query\n",
    "\n",
    "        # get all LMDB paths\n",
    "        self.lmdb_paths = self._get_all_lmdb_paths()\n",
    "\n",
    "        # map cluster_id to LMDB path\n",
    "        self.cluster_id_to_lmdb_path = self._map_cluster_ids_to_lmdb_path()\n",
    "\n",
    "        # get PDB IDs list for each cluster\n",
    "        self.cluster_ids = list(self.cluster_id_to_lmdb_path.keys())\n",
    "        self.cluster_id_to_pdb_ids = self._get_cluster_pdb_ids()\n",
    "\n",
    "    def _get_all_lmdb_paths(self):\n",
    "        \"\"\"get all LMDB paths in cluster_data_dir.\"\"\"\n",
    "        lmdb_files = []\n",
    "        for fname in os.listdir(self.cluster_data_dir):\n",
    "            path = os.path.join(self.cluster_data_dir, fname)\n",
    "            if os.path.isdir(path) and fname.endswith('.lmdb'):\n",
    "                lmdb_files.append(path)\n",
    "            elif os.path.isfile(path) and fname.endswith('.lmdb'):\n",
    "                lmdb_files.append(path)\n",
    "        return lmdb_files\n",
    "\n",
    "    def _map_cluster_ids_to_lmdb_path(self):\n",
    "        \"\"\"map cluster_id to LMDB path.\"\"\"\n",
    "        cluster_id_to_lmdb_path = {}\n",
    "        for lmdb_path in self.lmdb_paths:\n",
    "            basename = os.path.basename(lmdb_path)\n",
    "            # 假设命名规则为 cluster_{cluster_id}.lmdb\n",
    "            if basename.startswith('cluster_') and basename.endswith('.lmdb'):\n",
    "                cluster_id = basename[len('cluster_'):-len('.lmdb')]\n",
    "                cluster_id_to_lmdb_path[cluster_id] = lmdb_path\n",
    "        return cluster_id_to_lmdb_path\n",
    "\n",
    "    def _get_cluster_pdb_ids(self):\n",
    "        \"\"\"get PDB IDs list for each cluster.\"\"\"\n",
    "        cluster_id_to_pdb_ids = {}\n",
    "        for cluster_id, lmdb_path in self.cluster_id_to_lmdb_path.items():\n",
    "            env = lmdb.open(lmdb_path, readonly=True, lock=False)\n",
    "            pdb_ids = []\n",
    "            with env.begin() as txn:\n",
    "                cursor = txn.cursor()\n",
    "                for key, _ in cursor:\n",
    "                    pdb_id = key.decode()\n",
    "                    pdb_ids.append(pdb_id)\n",
    "            env.close()\n",
    "            cluster_id_to_pdb_ids[cluster_id] = pdb_ids\n",
    "        return cluster_id_to_pdb_ids\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"return the number of clusters (tasks).\"\"\"\n",
    "        return len(self.cluster_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        get a task (cluster), including support set and query set.\n",
    "\n",
    "        Args:\n",
    "            idx (int): index of the cluster.\n",
    "\n",
    "        Returns:\n",
    "            dict: a dictionary containing 'support_set' and 'query_set', with values as lists of data samples.\n",
    "        \"\"\"\n",
    "        cluster_id = self.cluster_ids[idx]\n",
    "        lmdb_path = self.cluster_id_to_lmdb_path[cluster_id]\n",
    "        pdb_ids = self.cluster_id_to_pdb_ids[cluster_id]\n",
    "\n",
    "        # ensure enough samples\n",
    "        total_samples_needed = self.k_shot + self.k_query\n",
    "        if len(pdb_ids) < total_samples_needed:\n",
    "            raise ValueError(f\"Not enough samples in cluster {cluster_id}\")\n",
    "\n",
    "        # randomly select support set and query set PDB IDs\n",
    "        pdb_ids_sampled = random.sample(pdb_ids, total_samples_needed)\n",
    "        support_pdb_ids = pdb_ids_sampled[:self.k_shot]\n",
    "        query_pdb_ids = pdb_ids_sampled[self.k_shot:]\n",
    "\n",
    "        # load support set data\n",
    "        support_set = []\n",
    "        env = lmdb.open(lmdb_path, readonly=True, lock=False)\n",
    "        with env.begin() as txn:\n",
    "            for pdb_id in support_pdb_ids:\n",
    "                data = txn.get(pdb_id.encode())\n",
    "                if data is not None:\n",
    "                    item = pickle.loads(data)\n",
    "                    protein_graph = item['protein_graph']\n",
    "                    ligand_graph = item['ligand_graph']\n",
    "                    kd_value = torch.tensor([item['kd_value']], dtype=torch.float)\n",
    "                    support_set.append((protein_graph, ligand_graph, kd_value, pdb_id))\n",
    "                else:\n",
    "                    env.close()\n",
    "                    raise KeyError(f\"PDB ID {pdb_id} not found in LMDB.\")\n",
    "\n",
    "        # load query set data\n",
    "        query_set = []\n",
    "        with env.begin() as txn:\n",
    "            for pdb_id in query_pdb_ids:\n",
    "                data = txn.get(pdb_id.encode())\n",
    "                if data is not None:\n",
    "                    item = pickle.loads(data)\n",
    "                    protein_graph = item['protein_graph']\n",
    "                    ligand_graph = item['ligand_graph']\n",
    "                    kd_value = torch.tensor([item['kd_value']], dtype=torch.float)\n",
    "                    query_set.append((protein_graph, ligand_graph, kd_value, pdb_id))\n",
    "                else:\n",
    "                    env.close()\n",
    "                    raise KeyError(f\"PDB ID {pdb_id} not found in LMDB.\")\n",
    "        env.close()\n",
    "\n",
    "        return {'support_set': support_set, 'query_set': query_set}\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    dataset = MetaDataset(cluster_data_dir='./cluster_data', k_shot=10, k_query=15)\n",
    "    print(len(dataset))\n",
    "    print(dataset[0])\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
